[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day1/index.html",
    "href": "posts/introduction-to-deep-learning-day1/index.html",
    "title": "深度学习入门(day1) - 线性回归",
    "section": "",
    "text": "深度学习入门(day1) - 线性回归\n\n线性回归的数学原理\n有一组数据 \\(\\mathbf{X} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1n} \\\\\nx_{21} & x_{22} & \\cdots & x_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{d1} & x_{d2} & \\cdots & x_{dn}\n\\end{bmatrix}\\) 和 \\(y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\)， 要如何找到一条由 \\(w = [w_1, w_2, w_3, \\dots, w_n]\\) 和 \\(b\\) 确定的曲线 \\(\\hat{y}=wx^T+b\\)，使得 \\(\\hat{y}\\) 尽可能的接近 \\(y\\)?\n\n如何表示预测值 \\(\\hat{y}\\) 与真实值 \\(y\\) 的接近程度？\n使用均方损失 \\(\\mathrm{\\ell}(X, y, w, b)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\hat{y_i})^2}=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\langle w, x_i \\rangle - b)^2}=\\frac{1}{2n}||y-(Xw+b)||^2\\) ，用 \\(\\frac{1}{2}\\) 抵消求导后的系数简化计算。\n如何根据损失更新 \\(w\\) 和 \\(b\\)？\n对于给定的 \\(X, y\\)，求出一组 \\(w^*, b^*\\) 使得 \\(\\mathrm{\\ell}(X, y, w, b)\\) 取得最小值：\\(w^*, b^*=\\underset{w, b}{\\arg\\max}{\\mathrm{\\ell}(X, y, w, b)}\\)。对于一些函数可以用偏导数为0解出极值点，但是大多数情况无解，所以要用到梯度下降算法： \\[ w_{k+1} = w_{k} - \\alpha\\frac{\\partial \\ell}{\\partial w}  \\] 将偏差b加入到权重简化计算：\\(X \\leftarrow \\begin{bmatrix} X \\\\ 1 \\end{bmatrix}\\)，\\(w \\leftarrow \\begin{bmatrix} w \\\\ b \\end{bmatrix}\\)\n\n\n\n线性回归从零开始实现\n首先导入包\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport torch\n\n生成训练数据集\n\n# 生成配置参数\ndef synthetic_data(w, b, num_examples):  #@save\n    \"\"\"生成y=Xw+b+噪声\"\"\"\n    X = torch.normal(0, 1, (num_examples, len(w)))\n    y = torch.matmul(X, w) + b\n    y += torch.normal(0, 0.01, y.shape)\n    return X, y.reshape((-1, 1))\n\ntrue_w = torch.tensor([2.4, -2.1])\ntrue_b = 4.1\nfeatures, labels = synthetic_data(true_w, true_b, 1000)\n\n可视化特征与目标值的关系\n\ndf = pd.DataFrame(features, columns=[f'x{i+1}' for i in range(len(true_w))])\ndf['y'] = labels\n\nimport matplotlib.pyplot as plt\n\n# 设置统一画布尺寸（单位英寸）\nSINGLE_FIGSIZE = (7, 4)\n\nfor feature in df.columns[0:-1]:\n    # 创建独立画布（每次循环新建Figure对象）\n    plt.figure(figsize=SINGLE_FIGSIZE, dpi=100)\n    \n    # 绘制散点图（优化点样式）\n    plt.scatter(df[feature], df['y'], \n                alpha=0.6,          # 点透明度\n                edgecolor='w',      # 点边缘颜色\n                s=45,              # 点大小 \n                c=df[feature],      # 颜色映射特征值\n                cmap='viridis')     # 使用清晰的颜色映射\n    \n    # 添加统计信息（参考网页3的标注技巧）\n    corr = df[feature].corr(df['y'])\n    plt.text(0.05, 0.92, f'Pearson R = {corr:.2f}', \n             transform=plt.gca().transAxes,\n             fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n    \n    # 设置坐标标签和标题\n    plt.xlabel(feature.upper(), fontsize=14)\n    plt.ylabel('Target Y', fontsize=14)\n    plt.title(f'{feature} vs Y Relationship', fontsize=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n小批量读取数据集\n\nimport random\nimport torch\n\ndef data_iter(batch_size, features, labels):\n    if len(features) != len(labels):\n        print(\"len(features) != len(labels)\")\n        raise IndexError\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)\n    for i in range(0, num_examples, batch_size):\n        batch_indices = torch.tensor(\n            indices[i:min(i+batch_size, num_examples)]\n        )\n        yield features[batch_indices], labels[batch_indices]\n\n初始化模型参数w ,b\n\n# w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) # 长度为2的列向量\nw = torch.zeros((2, 1), requires_grad=True)\nb = torch.zeros(1, requires_grad=True)\n\n定义线性回归模型\n\ndef linreg(X, w, b):\n    return torch.matmul(X, w) + b\n\n定义损失函数 \\(\\mathrm{\\ell}(X, y, w, b)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\hat{y_i})^2}\\)\n\ndef squared_loss(y_hat, y):\n    if len(y) != len(y_hat):\n        print(\"len(y) != len(y_hat)\")\n        raise ValueError\n    return ((y_hat - y.reshape(y_hat.shape)) ** 2).mean() / 2\n\n定义梯度下降算法\n\ndef sgd(params, lr):\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad\n            param.grad.zero_()\n\n开始训练\n\nbatch_size = 10\nlr = 0.03\nnum_epochs = 3\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epochs):\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y)\n        l.backward()\n        sgd([w, b], lr)\n    with torch.no_grad():\n        train_l = loss(net(features, w, b), labels)\n        print(f'epoch {epoch+1}, loss {float(train_l.mean()):f}')\n\nepoch 1, loss 0.026908\nepoch 2, loss 0.000106\nepoch 3, loss 0.000052\n\n\n\nprint(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\nprint(f'b的估计误差: {true_b - b}')\n\nw的估计误差: tensor([-0.0003, -0.0003], grad_fn=&lt;SubBackward0&gt;)\nb的估计误差: tensor([-0.0003], grad_fn=&lt;RsubBackward1&gt;)\n\n\n\n\n总结\n学习了基础的线性回归模型，当加入更多参数时，根据参数的取值范围和相关性，线性回归无法解决问题，需进一步了解数据预处理以及梯度下降算法。"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quarto_website",
    "section": "",
    "text": "深度学习入门(day1) - 线性回归\n\n\n\n\n\n\n\n\nMay 20, 2025\n\n\nTUT\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 20, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 17, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]