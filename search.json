[
  {
    "objectID": "posts/introduction-to-deep-learning-day2/index.html",
    "href": "posts/introduction-to-deep-learning-day2/index.html",
    "title": "深度学习入门(day2) - softmax回归",
    "section": "",
    "text": "假设输入是一个 2\\times2 的灰度图像。每个图象对于四个特征 x_1, x_2, x_3, x_4。此外，假设每个图像都属于“猫”，“狗”，“鸡”中的一个。使用独热编码表示类别：(1, 0, 0)对应猫、(0, 1, 0)对应狗、(0, 0, 1)对应鸡： y \\in \\{ (1, 0, 0), (0, 1, 0), (0, 0, 1)\\} \n\n\n\n首先未每个输入计算三个未规范化的预测：o_1, o_2 \\text{和} o_3 \n\\begin{aligned}\no_1 &= x_1w_{11} + x_2w_{12} + x_3w_{13} + x_4w_{14} + b \\\\\no_2 &= x_1w_{21} + x_2w_{22} + x_3w_{23} + x_4w_{24} + b \\\\\no_3 &= x_1w_{31} + x_2w_{32} + x_3w_{33} + x_4w_{34} + b\n\\end{aligned}\n\n用神经网络图来描述计算过程\n\n\n\n将权重放到 3\\times4 的矩阵中，以上公式可以表示为：o = Wx + b。\n\n\n\n通过softmax运算可以将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。我们首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式： \n\\hat{y} = \\text{softmax}(o) \\quad 其中 \\quad \\hat{y_i} = \\frac{\\exp{(o_j)}}{\\sum_{k}\\exp{(o_k)}}\n\n\n\n\n使用交叉熵来衡量预测概率 \\hat{y} 与 y 的区别，公式可以表示为： \nL = -\\sum_{i=1}^{n} y_i \\log(\\hat{y_i})\n\n其中 \\hat{y_i} = \\frac{e^{o_i}}{\\sum_{j=1}^n e^{o_j}} 是softmax函数的输出。\n\n\n\n设 z_k 是最后一层的输入，求 \\frac{\\partial L}{\\partial o_k} ：\n1. 展开交叉熵损失： \nL = -\\sum_{i=1}^n y_i \\log\\left(\\frac{e^{o_i}}{\\sum_{j=1}^n e^{o_j}}\\right)\n\n2. 化简对数项： \nL = -\\sum_{i=1}^n y_i \\left(o_i - \\log\\left(\\sum_{j=1}^n e^{o_j}\\right)\\right)\n\n3. 分离求和项： \nL = -\\sum_{i=1}^n y_i o_i + \\log\\left(\\sum_{j=1}^n e^{o_j}\\right) \\cdot \\underbrace{\\sum_{i=1}^n y_i}_{=1}\n\n4. 对 z_k 求导数： \n\\frac{\\partial L}{\\partial o_k} = -y_k + \\frac{e^{o_k}}{\\sum_{j=1}^n e^{o_j}} = \\text{softmax}(o_k) - y_k\n\n—\n#### 最终梯度表达式\n\n\\boxed{\n\\frac{\\partial L}{\\partial o_k} =\n\\begin{cases}\n\\text{softmax}(o_k) - 1 & \\text{如果 } y_k = 1 \\\\\n\\text{softmax}(o_k) & \\text{其他情况}\n\\end{cases}\n}\n\n\n注：当且仅当 y_k 是真实类别时导数为 \\text{softmax}(o_k)-1，其他情况导数为 \\text{softmax}(o_k)。即真实类别时梯度为负，其他情况为正，优化是往正确概率的方向调整。\n\n\n\n\n\n\n\n\n使用内置的Fashion-MNIST数据集\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'  # 在 Jupyter 中设置矢量图显示\nimport torchvision\nfrom torchvision import transforms\nimport torch\nfrom torch.utils import data\n\n定义加载mnist数据集函数\n\ndef get_dataloader_workers():\n    return 4\n\ndef load_data_fashion_mnist(batch_size, resize=None):\n    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n    trans = [transforms.ToTensor()]\n    if resize:\n        trans.insert(0, transforms.Resize(resize))\n    trans = transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=True, transform=trans, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=False, transform=trans, download=True)\n    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n                            num_workers=get_dataloader_workers()),\n            data.DataLoader(mnist_test, batch_size, shuffle=False,\n                            num_workers=get_dataloader_workers()))\n\n\nbatch_size = 256\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size)\n\n初始化模型参数，由于要将输入的图片展开为一维，W 矩阵的形状为 784\\times10\n\nnum_inputs = 784\nnum_outputs = 10\n\nW = torch.normal(0, 0.1, size=(num_inputs, num_outputs), requires_grad=True)\nb = torch.zeros(num_outputs, requires_grad=True)\n\n\n\n\n\ndef softmax(X):\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepdim=True) # 对行求和，并保留维度，partition矩阵的每一个元素都是行和。\n    return X_exp / partition\n\n上述代码将X的矩阵每个元素变成元素非负，且行和为1的矩阵。\n\nX = torch.normal(0, 1, (2, 5))\nX_prob = softmax(X)\nX_prob, X_prob.sum(1)\n\n(tensor([[0.4655, 0.0943, 0.2297, 0.0336, 0.1770],\n         [0.1538, 0.0851, 0.3993, 0.1771, 0.1848]]),\n tensor([1., 1.]))\n\n\n\n注：矩阵中非常大或者非常小的元素可能会造成数值上溢或下溢，代码并没有措施防止这点。\n\n\n\n\n模型为线性回归与softmax运算的组合\n\ndef net(X):\n    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)\n\n\n\n\n使用花式索引，以y作为y_hat中概率的索引，快速选择 \\hat{y_i}\n\ny = torch.tensor([0, 2, 1])\ny_hat = torch.tensor([[0.6, 0.3, 0.1], [0.3, 0.2, 0.5], [0.3, 0.4, 0.3]])\ny_hat[range(len(y_hat)), y]\n\ntensor([0.6000, 0.5000, 0.4000])\n\n\n一行代码就可以实现交叉熵损失函数\n\ndef cross_entropy(y_hat, y):\n    return -torch.log(y_hat[range(len(y_hat)), y]) # 化简后的交叉熵形式\n\ncross_entropy(y_hat, y)\n\ntensor([0.5108, 0.6931, 0.9163])\n\n\n\n\n\n用正确预测数量与总预测数量之比评估准确率，\\hat{y} 是一个存储概率的矩阵，利用torch.argmax()方法找出最大概率值的列号，与 y 进行对比。\n\ndef accuracy(y_hat, y):\n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1: # y_hat 为矩阵\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y\n    return float(cmp.type(y.dtype).sum())\n\n准确率为\n\naccuracy(y_hat, y) / len(y)\n\n1.0\n\n\n对于任意的迭代器data_iter，我们可以评估在任意模型net的精度。\n\n# 先定义一个累加器工具\nclass Accumulator:\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n # 定义长度为n的列表\n    \n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)] # 列表与输入值累加\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx] # 重写[]方法\n\ndef evaluate_accuracy(net, data_iter):\n    \"\"\"计算在指定数据集上模型的精度\"\"\"\n    metric = Accumulator(2) # [\"正确预测数\", \"预测总数\"]\n    with torch.no_grad():\n        for X, y in data_iter:\n            metric.add(accuracy(net(X), y), y.numel())\n    return metric[0] / metric[1]\n\nevaluate_accuracy(net, test_iter)\n\n0.1229\n\n\n\n\n\n首先，定义一个函数来训练一个迭代周期，updater是更新模型参数的常用函数，这里使用梯度下降算法。\n\ndef sgd(params, lr, batch_size):\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad / batch_size # 除以批量大小等价于损失函数中求批量的均值\n            param.grad.zero_()\n\nlr = 0.1\n\ndef updater(batch_size):\n    return sgd([W, b], lr, batch_size)\n\ndef train_epoch(net, train_iter, loss, updater):\n    \"\"\"训练模型一个迭代周期\"\"\"\n    metric = Accumulator(3) # 训练损失总和、训练准确度总和、样本数\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        l.sum().backward()\n        updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练精度\n    return metric[0] / metric[2], metric[1] / metric[2]\n    \n\n训练之前，定义一个在动画中绘制数据的实用程序类Animator，它可以可视化训练过程。\n\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\nclass Animator:\n    \"\"\"在动画中绘制数据\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        \n        # 创建图形和坐标轴\n        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes]  # 统一为列表格式方便后续操作\n            \n        # 坐标轴配置函数（原生替代d2l.set_axes）\n        def config_axes(ax):\n            ax.set_xlabel(xlabel)\n            ax.set_ylabel(ylabel)\n            if xlim: ax.set_xlim(xlim)\n            if ylim: ax.set_ylim(ylim)\n            ax.set_xscale(xscale)\n            ax.set_yscale(yscale)\n            if legend: ax.legend(legend)\n            ax.grid()\n            \n        self.config_axes = lambda: config_axes(self.axes[0])  # 绑定配置函数\n        \n        # 初始化数据容器\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        # 处理单值输入（兼容标量输入）\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        \n        # 处理x输入（兼容标量输入）\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n  # 所有y序列共享相同x值\n            \n        # 初始化数据存储列表\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n            \n        # 添加数据点\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        \n        # 清除当前图形并重绘\n        self.axes[0].cla()\n        for x_data, y_data, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x_data, y_data, fmt)\n            \n        # 配置坐标轴\n        self.config_axes()\n        \n        # 动态显示图形（需在Jupyter中运行）\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n实现一个训练函数，该训练函数将会运行多个迭代周期（由num_epochs指定）。在每个迭代周期结束时，利用test_iter访问到的测试数据集对模型进行评估。最后利用Animator类来可视化训练进度。\n\ndef train(net, train_iter, test_iter, loss, num_epochs, updater):\n    \"\"\"训练模型\"\"\"\n    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n                        legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        train_metrics = train_epoch(net, train_iter, loss, updater)\n        test_acc = evaluate_accuracy(net, test_iter)\n        animator.add(epoch + 1, train_metrics + (test_acc,))\n        \n    train_loss, train_acc = train_metrics\n    assert train_loss &lt; 0.5, train_loss\n    assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc\n    assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc\n\n训练10个迭代周期，学习率0.1\n\nnum_epoch = 10\ntrain(net, train_iter, test_iter, cross_entropy, num_epoch, updater)\n\n\n\n\n\n\n\n\n\n\n\n训练完成后，使用模型对图片进行分类预测，比较实际标签和预测。\n\ndef get_fashion_mnist_labels(labels):\n    \"\"\"返回Fashion-MNIST数据集的文本标签\"\"\"\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]\n\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n    \"\"\"绘制图像列表\"\"\"\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n    axes = axes.flatten()\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        ax.imshow(img.numpy())\n        ax.axes.get_xaxis().set_visible(False)\n        ax.axes.get_yaxis().set_visible(False)\n        if titles:\n            ax.set_title(titles[i])\n    return axes\n\ndef predict(net, test_iter, n=6):\n    \"\"\"预测标签\"\"\"\n    for X, y in test_iter:\n        break\n    trues = get_fashion_mnist_labels(y)\n    preds = get_fashion_mnist_labels(net(X).argmax(axis=1))\n    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n    show_images(\n        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n]\n    )\n\npredict(net, test_iter)\n\n\n\n\n\n\n\n\n\n\n\n\n训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day2/index.html#softmax回归的数学原理",
    "href": "posts/introduction-to-deep-learning-day2/index.html#softmax回归的数学原理",
    "title": "深度学习入门(day2) - softmax回归",
    "section": "",
    "text": "假设输入是一个 2\\times2 的灰度图像。每个图象对于四个特征 x_1, x_2, x_3, x_4。此外，假设每个图像都属于“猫”，“狗”，“鸡”中的一个。使用独热编码表示类别：(1, 0, 0)对应猫、(0, 1, 0)对应狗、(0, 0, 1)对应鸡： y \\in \\{ (1, 0, 0), (0, 1, 0), (0, 0, 1)\\} \n\n\n\n首先未每个输入计算三个未规范化的预测：o_1, o_2 \\text{和} o_3 \n\\begin{aligned}\no_1 &= x_1w_{11} + x_2w_{12} + x_3w_{13} + x_4w_{14} + b \\\\\no_2 &= x_1w_{21} + x_2w_{22} + x_3w_{23} + x_4w_{24} + b \\\\\no_3 &= x_1w_{31} + x_2w_{32} + x_3w_{33} + x_4w_{34} + b\n\\end{aligned}\n\n用神经网络图来描述计算过程\n\n\n\n将权重放到 3\\times4 的矩阵中，以上公式可以表示为：o = Wx + b。\n\n\n\n通过softmax运算可以将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。我们首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式： \n\\hat{y} = \\text{softmax}(o) \\quad 其中 \\quad \\hat{y_i} = \\frac{\\exp{(o_j)}}{\\sum_{k}\\exp{(o_k)}}\n\n\n\n\n使用交叉熵来衡量预测概率 \\hat{y} 与 y 的区别，公式可以表示为： \nL = -\\sum_{i=1}^{n} y_i \\log(\\hat{y_i})\n\n其中 \\hat{y_i} = \\frac{e^{o_i}}{\\sum_{j=1}^n e^{o_j}} 是softmax函数的输出。\n\n\n\n设 z_k 是最后一层的输入，求 \\frac{\\partial L}{\\partial o_k} ：\n1. 展开交叉熵损失： \nL = -\\sum_{i=1}^n y_i \\log\\left(\\frac{e^{o_i}}{\\sum_{j=1}^n e^{o_j}}\\right)\n\n2. 化简对数项： \nL = -\\sum_{i=1}^n y_i \\left(o_i - \\log\\left(\\sum_{j=1}^n e^{o_j}\\right)\\right)\n\n3. 分离求和项： \nL = -\\sum_{i=1}^n y_i o_i + \\log\\left(\\sum_{j=1}^n e^{o_j}\\right) \\cdot \\underbrace{\\sum_{i=1}^n y_i}_{=1}\n\n4. 对 z_k 求导数： \n\\frac{\\partial L}{\\partial o_k} = -y_k + \\frac{e^{o_k}}{\\sum_{j=1}^n e^{o_j}} = \\text{softmax}(o_k) - y_k\n\n—\n#### 最终梯度表达式\n\n\\boxed{\n\\frac{\\partial L}{\\partial o_k} =\n\\begin{cases}\n\\text{softmax}(o_k) - 1 & \\text{如果 } y_k = 1 \\\\\n\\text{softmax}(o_k) & \\text{其他情况}\n\\end{cases}\n}\n\n\n注：当且仅当 y_k 是真实类别时导数为 \\text{softmax}(o_k)-1，其他情况导数为 \\text{softmax}(o_k)。即真实类别时梯度为负，其他情况为正，优化是往正确概率的方向调整。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day2/index.html#softmax回归的代码实现",
    "href": "posts/introduction-to-deep-learning-day2/index.html#softmax回归的代码实现",
    "title": "深度学习入门(day2) - softmax回归",
    "section": "",
    "text": "使用内置的Fashion-MNIST数据集\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'  # 在 Jupyter 中设置矢量图显示\nimport torchvision\nfrom torchvision import transforms\nimport torch\nfrom torch.utils import data\n\n定义加载mnist数据集函数\n\ndef get_dataloader_workers():\n    return 4\n\ndef load_data_fashion_mnist(batch_size, resize=None):\n    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n    trans = [transforms.ToTensor()]\n    if resize:\n        trans.insert(0, transforms.Resize(resize))\n    trans = transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=True, transform=trans, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=False, transform=trans, download=True)\n    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n                            num_workers=get_dataloader_workers()),\n            data.DataLoader(mnist_test, batch_size, shuffle=False,\n                            num_workers=get_dataloader_workers()))\n\n\nbatch_size = 256\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size)\n\n初始化模型参数，由于要将输入的图片展开为一维，W 矩阵的形状为 784\\times10\n\nnum_inputs = 784\nnum_outputs = 10\n\nW = torch.normal(0, 0.1, size=(num_inputs, num_outputs), requires_grad=True)\nb = torch.zeros(num_outputs, requires_grad=True)\n\n\n\n\n\ndef softmax(X):\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepdim=True) # 对行求和，并保留维度，partition矩阵的每一个元素都是行和。\n    return X_exp / partition\n\n上述代码将X的矩阵每个元素变成元素非负，且行和为1的矩阵。\n\nX = torch.normal(0, 1, (2, 5))\nX_prob = softmax(X)\nX_prob, X_prob.sum(1)\n\n(tensor([[0.4655, 0.0943, 0.2297, 0.0336, 0.1770],\n         [0.1538, 0.0851, 0.3993, 0.1771, 0.1848]]),\n tensor([1., 1.]))\n\n\n\n注：矩阵中非常大或者非常小的元素可能会造成数值上溢或下溢，代码并没有措施防止这点。\n\n\n\n\n模型为线性回归与softmax运算的组合\n\ndef net(X):\n    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)\n\n\n\n\n使用花式索引，以y作为y_hat中概率的索引，快速选择 \\hat{y_i}\n\ny = torch.tensor([0, 2, 1])\ny_hat = torch.tensor([[0.6, 0.3, 0.1], [0.3, 0.2, 0.5], [0.3, 0.4, 0.3]])\ny_hat[range(len(y_hat)), y]\n\ntensor([0.6000, 0.5000, 0.4000])\n\n\n一行代码就可以实现交叉熵损失函数\n\ndef cross_entropy(y_hat, y):\n    return -torch.log(y_hat[range(len(y_hat)), y]) # 化简后的交叉熵形式\n\ncross_entropy(y_hat, y)\n\ntensor([0.5108, 0.6931, 0.9163])\n\n\n\n\n\n用正确预测数量与总预测数量之比评估准确率，\\hat{y} 是一个存储概率的矩阵，利用torch.argmax()方法找出最大概率值的列号，与 y 进行对比。\n\ndef accuracy(y_hat, y):\n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1: # y_hat 为矩阵\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y\n    return float(cmp.type(y.dtype).sum())\n\n准确率为\n\naccuracy(y_hat, y) / len(y)\n\n1.0\n\n\n对于任意的迭代器data_iter，我们可以评估在任意模型net的精度。\n\n# 先定义一个累加器工具\nclass Accumulator:\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n # 定义长度为n的列表\n    \n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)] # 列表与输入值累加\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx] # 重写[]方法\n\ndef evaluate_accuracy(net, data_iter):\n    \"\"\"计算在指定数据集上模型的精度\"\"\"\n    metric = Accumulator(2) # [\"正确预测数\", \"预测总数\"]\n    with torch.no_grad():\n        for X, y in data_iter:\n            metric.add(accuracy(net(X), y), y.numel())\n    return metric[0] / metric[1]\n\nevaluate_accuracy(net, test_iter)\n\n0.1229\n\n\n\n\n\n首先，定义一个函数来训练一个迭代周期，updater是更新模型参数的常用函数，这里使用梯度下降算法。\n\ndef sgd(params, lr, batch_size):\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad / batch_size # 除以批量大小等价于损失函数中求批量的均值\n            param.grad.zero_()\n\nlr = 0.1\n\ndef updater(batch_size):\n    return sgd([W, b], lr, batch_size)\n\ndef train_epoch(net, train_iter, loss, updater):\n    \"\"\"训练模型一个迭代周期\"\"\"\n    metric = Accumulator(3) # 训练损失总和、训练准确度总和、样本数\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        l.sum().backward()\n        updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练精度\n    return metric[0] / metric[2], metric[1] / metric[2]\n    \n\n训练之前，定义一个在动画中绘制数据的实用程序类Animator，它可以可视化训练过程。\n\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\nclass Animator:\n    \"\"\"在动画中绘制数据\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        \n        # 创建图形和坐标轴\n        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes]  # 统一为列表格式方便后续操作\n            \n        # 坐标轴配置函数（原生替代d2l.set_axes）\n        def config_axes(ax):\n            ax.set_xlabel(xlabel)\n            ax.set_ylabel(ylabel)\n            if xlim: ax.set_xlim(xlim)\n            if ylim: ax.set_ylim(ylim)\n            ax.set_xscale(xscale)\n            ax.set_yscale(yscale)\n            if legend: ax.legend(legend)\n            ax.grid()\n            \n        self.config_axes = lambda: config_axes(self.axes[0])  # 绑定配置函数\n        \n        # 初始化数据容器\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        # 处理单值输入（兼容标量输入）\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        \n        # 处理x输入（兼容标量输入）\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n  # 所有y序列共享相同x值\n            \n        # 初始化数据存储列表\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n            \n        # 添加数据点\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        \n        # 清除当前图形并重绘\n        self.axes[0].cla()\n        for x_data, y_data, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x_data, y_data, fmt)\n            \n        # 配置坐标轴\n        self.config_axes()\n        \n        # 动态显示图形（需在Jupyter中运行）\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n实现一个训练函数，该训练函数将会运行多个迭代周期（由num_epochs指定）。在每个迭代周期结束时，利用test_iter访问到的测试数据集对模型进行评估。最后利用Animator类来可视化训练进度。\n\ndef train(net, train_iter, test_iter, loss, num_epochs, updater):\n    \"\"\"训练模型\"\"\"\n    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n                        legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        train_metrics = train_epoch(net, train_iter, loss, updater)\n        test_acc = evaluate_accuracy(net, test_iter)\n        animator.add(epoch + 1, train_metrics + (test_acc,))\n        \n    train_loss, train_acc = train_metrics\n    assert train_loss &lt; 0.5, train_loss\n    assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc\n    assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc\n\n训练10个迭代周期，学习率0.1\n\nnum_epoch = 10\ntrain(net, train_iter, test_iter, cross_entropy, num_epoch, updater)\n\n\n\n\n\n\n\n\n\n\n\n训练完成后，使用模型对图片进行分类预测，比较实际标签和预测。\n\ndef get_fashion_mnist_labels(labels):\n    \"\"\"返回Fashion-MNIST数据集的文本标签\"\"\"\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]\n\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n    \"\"\"绘制图像列表\"\"\"\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n    axes = axes.flatten()\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        ax.imshow(img.numpy())\n        ax.axes.get_xaxis().set_visible(False)\n        ax.axes.get_yaxis().set_visible(False)\n        if titles:\n            ax.set_title(titles[i])\n    return axes\n\ndef predict(net, test_iter, n=6):\n    \"\"\"预测标签\"\"\"\n    for X, y in test_iter:\n        break\n    trues = get_fashion_mnist_labels(y)\n    preds = get_fashion_mnist_labels(net(X).argmax(axis=1))\n    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n    show_images(\n        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n]\n    )\n\npredict(net, test_iter)"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day2/index.html#总结",
    "href": "posts/introduction-to-deep-learning-day2/index.html#总结",
    "title": "深度学习入门(day2) - softmax回归",
    "section": "",
    "text": "训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "notebook",
    "section": "",
    "text": "深度学习入门(day2) - softmax回归\n\n\n回归除了可以用于预测多少的问题，也可以用于解决分类问题。\n\n\n\n\n\nMay 22, 2025\n\n\nTUT\n\n\n\n\n\n\n\n\n\n\n\n\n深度学习入门(day1) - 线性回归\n\n\n通过了解线性回归的数学原理，用python代码从零开始实现简单的线性回归模型。\n\n\n\n\n\nMay 20, 2025\n\n\nTUT\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day1/index.html",
    "href": "posts/introduction-to-deep-learning-day1/index.html",
    "title": "深度学习入门(day1) - 线性回归",
    "section": "",
    "text": "线性回归基于几个简单的假设：首先，假设自变量 x 和因变量 y 之间的关系是线性的，即 y 可以表示为 x 中元素的加权和，这里通常允许包含观测值的一些噪声；其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。\n有一组数据 \\mathbf{X} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1n} \\\\\nx_{21} & x_{22} & \\cdots & x_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{d1} & x_{d2} & \\cdots & x_{dn}\n\\end{bmatrix} 和 y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} 要如何找到一条由 w = [w_1, w_2, w_3, \\dots, w_n] 和 b 确定的曲线 \\hat{y}=wx^T+b，使得 \\hat{y} 尽可能的接近 y?\n\n如何表示预测值 \\hat{y} 与真实值 y 的接近程度？\n使用均方损失 \\mathrm{\\ell}(X, y, w, b)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\hat{y_i})^2}=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\langle w, x_i \\rangle - b)^2}=\\frac{1}{2n}||y-(Xw+b)||^2 ，用 \\frac{1}{2} 抵消求导后的系数简化计算。\n如何根据损失更新 w 和 b？\n对于给定的 X, y，求出一组 w^*, b^* 使得 \\mathrm{\\ell}(X, y, w, b) 取得最小值：w^*, b^*=\\underset{w, b}{\\arg\\max}{\\mathrm{\\ell}(X, y, w, b)}。对于一些函数可以用偏导数为0解出极值点，但是大多数情况无解，所以要用到梯度下降算法：  w_{k+1} = w_{k} - \\alpha\\frac{\\partial \\ell}{\\partial w}   将偏差b加入到权重简化计算：X \\leftarrow \\begin{bmatrix} X \\\\ 1 \\end{bmatrix}，w \\leftarrow \\begin{bmatrix} w \\\\ b \\end{bmatrix}\n\n\n\n\n\n\n首先导入包\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nimport numpy as np\nimport pandas as pd\nimport torch\n\n生成训练数据集\n\n# 生成配置参数\ndef synthetic_data(w, b, num_examples):  #@save\n    \"\"\"生成y=Xw+b+噪声\"\"\"\n    X = torch.normal(0, 1, (num_examples, len(w)))\n    y = torch.matmul(X, w) + b\n    y += torch.normal(0, 0.01, y.shape)\n    return X, y.reshape((-1, 1))\n\ntrue_w = torch.tensor([2.4, -2.1])\ntrue_b = 4.1\nfeatures, labels = synthetic_data(true_w, true_b, 1000)\n\n可视化特征与目标值的关系\n\ndf = pd.DataFrame(features, columns=[f'x{i+1}' for i in range(len(true_w))])\ndf['y'] = labels\n\nimport matplotlib.pyplot as plt\n\ndef show_scatter(df):\n    features = df.columns[0:-1]  # 获取所有特征列名\n\n    # 创建组合画布（宽度增大适应横向布局）\n    fig, axes = plt.subplots(\n        nrows=1,  # 单行布局\n        ncols=len(features),  # 列数等于特征数量\n        figsize=(6 * len(features), 3.5),  # 动态宽度适配特征数量\n        dpi=100,\n        constrained_layout=True  # 自动调整子图间距\n    )\n\n    # 并行循环特征与对应的坐标轴\n    for ax, feature in zip(axes, features):\n        # 在指定坐标轴上绘制散点图\n        sc = ax.scatter(\n            df[feature], df['y'], \n            alpha=0.6,\n            edgecolor='w',\n            s=45, \n            c=df[feature],\n            cmap='viridis'\n        )\n        \n        # 添加颜色条（共享同一刻度）\n        if ax == axes[-1]:  # 只在最后一个坐标轴添加颜色条\n            plt.colorbar(sc, ax=axes, label=feature.upper())\n        \n        # 添加统计信息\n        corr = df[feature].corr(df['y'])\n        ax.text(\n            0.05, 0.92, \n            f'Pearson R = {corr:.2f}',\n            transform=ax.transAxes,\n            fontsize=12,\n            bbox=dict(facecolor='white', alpha=0.8)\n        )\n        \n        # 设置坐标标签\n        ax.set_xlabel(feature.upper(), fontsize=14)\n        ax.set_ylabel('Target Y', fontsize=14)\n        ax.set_title(f'{feature} vs Y', fontsize=16)\n\n    # 调整整体标题（可选）\n    fig.suptitle('Feature-Target Relationships', fontsize=18, y=1.02)\n    plt.show()\n    \nshow_scatter(df)\n\n\n\n\n\n\n\n\n小批量读取数据集\n\nimport random\nimport torch\n\ndef data_iter(batch_size, features, labels):\n    if len(features) != len(labels):\n        print(\"len(features) != len(labels)\")\n        raise IndexError\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)\n    for i in range(0, num_examples, batch_size):\n        batch_indices = torch.tensor(\n            indices[i:min(i+batch_size, num_examples)]\n        )\n        yield features[batch_indices], labels[batch_indices]\n\n\n\n\n初始化模型参数w ,b\n\n# w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) # 长度为2的列向量\nw = torch.zeros((2, 1), requires_grad=True)\nb = torch.zeros(1, requires_grad=True)\n\n定义线性回归模型\n\ndef linreg(X, w, b):\n    return torch.matmul(X, w) + b\n\n\n\n\n\\mathrm{\\ell}(X, y, w, b)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\hat{y_i})^2}\n\ndef squared_loss(y_hat, y):\n    if len(y) != len(y_hat):\n        print(\"len(y) != len(y_hat)\")\n        raise ValueError\n    return ((y_hat - y.reshape(y_hat.shape)) ** 2).mean() / 2\n\n\n\n\n\ndef sgd(params, lr):\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad\n            param.grad.zero_()\n\n\n\n\n\nbatch_size = 10\nlr = 0.03\nnum_epochs = 3\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epochs):\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y)\n        l.backward()\n        sgd([w, b], lr)\n    with torch.no_grad():\n        train_l = loss(net(features, w, b), labels)\n        print(f'epoch {epoch+1}, loss {float(train_l.mean()):f}')\n\nepoch 1, loss 0.029633\nepoch 2, loss 0.000117\nepoch 3, loss 0.000049\n\n\n\nprint(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\nprint(f'b的估计误差: {true_b - b}')\n\nw的估计误差: tensor([ 3.9816e-05, -1.4021e-03], grad_fn=&lt;SubBackward0&gt;)\nb的估计误差: tensor([0.0001], grad_fn=&lt;RsubBackward1&gt;)\n\n\n\n\n\n\n学习了基础的线性回归模型，当加入更多参数时，根据参数的取值范围和相关性，线性回归无法解决问题，需进一步了解数据预处理以及梯度下降算法。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day1/index.html#线性回归的数学原理",
    "href": "posts/introduction-to-deep-learning-day1/index.html#线性回归的数学原理",
    "title": "深度学习入门(day1) - 线性回归",
    "section": "",
    "text": "线性回归基于几个简单的假设：首先，假设自变量 x 和因变量 y 之间的关系是线性的，即 y 可以表示为 x 中元素的加权和，这里通常允许包含观测值的一些噪声；其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。\n有一组数据 \\mathbf{X} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1n} \\\\\nx_{21} & x_{22} & \\cdots & x_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{d1} & x_{d2} & \\cdots & x_{dn}\n\\end{bmatrix} 和 y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} 要如何找到一条由 w = [w_1, w_2, w_3, \\dots, w_n] 和 b 确定的曲线 \\hat{y}=wx^T+b，使得 \\hat{y} 尽可能的接近 y?\n\n如何表示预测值 \\hat{y} 与真实值 y 的接近程度？\n使用均方损失 \\mathrm{\\ell}(X, y, w, b)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\hat{y_i})^2}=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\langle w, x_i \\rangle - b)^2}=\\frac{1}{2n}||y-(Xw+b)||^2 ，用 \\frac{1}{2} 抵消求导后的系数简化计算。\n如何根据损失更新 w 和 b？\n对于给定的 X, y，求出一组 w^*, b^* 使得 \\mathrm{\\ell}(X, y, w, b) 取得最小值：w^*, b^*=\\underset{w, b}{\\arg\\max}{\\mathrm{\\ell}(X, y, w, b)}。对于一些函数可以用偏导数为0解出极值点，但是大多数情况无解，所以要用到梯度下降算法：  w_{k+1} = w_{k} - \\alpha\\frac{\\partial \\ell}{\\partial w}   将偏差b加入到权重简化计算：X \\leftarrow \\begin{bmatrix} X \\\\ 1 \\end{bmatrix}，w \\leftarrow \\begin{bmatrix} w \\\\ b \\end{bmatrix}"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day1/index.html#线性回归代码实现",
    "href": "posts/introduction-to-deep-learning-day1/index.html#线性回归代码实现",
    "title": "深度学习入门(day1) - 线性回归",
    "section": "",
    "text": "首先导入包\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nimport numpy as np\nimport pandas as pd\nimport torch\n\n生成训练数据集\n\n# 生成配置参数\ndef synthetic_data(w, b, num_examples):  #@save\n    \"\"\"生成y=Xw+b+噪声\"\"\"\n    X = torch.normal(0, 1, (num_examples, len(w)))\n    y = torch.matmul(X, w) + b\n    y += torch.normal(0, 0.01, y.shape)\n    return X, y.reshape((-1, 1))\n\ntrue_w = torch.tensor([2.4, -2.1])\ntrue_b = 4.1\nfeatures, labels = synthetic_data(true_w, true_b, 1000)\n\n可视化特征与目标值的关系\n\ndf = pd.DataFrame(features, columns=[f'x{i+1}' for i in range(len(true_w))])\ndf['y'] = labels\n\nimport matplotlib.pyplot as plt\n\ndef show_scatter(df):\n    features = df.columns[0:-1]  # 获取所有特征列名\n\n    # 创建组合画布（宽度增大适应横向布局）\n    fig, axes = plt.subplots(\n        nrows=1,  # 单行布局\n        ncols=len(features),  # 列数等于特征数量\n        figsize=(6 * len(features), 3.5),  # 动态宽度适配特征数量\n        dpi=100,\n        constrained_layout=True  # 自动调整子图间距\n    )\n\n    # 并行循环特征与对应的坐标轴\n    for ax, feature in zip(axes, features):\n        # 在指定坐标轴上绘制散点图\n        sc = ax.scatter(\n            df[feature], df['y'], \n            alpha=0.6,\n            edgecolor='w',\n            s=45, \n            c=df[feature],\n            cmap='viridis'\n        )\n        \n        # 添加颜色条（共享同一刻度）\n        if ax == axes[-1]:  # 只在最后一个坐标轴添加颜色条\n            plt.colorbar(sc, ax=axes, label=feature.upper())\n        \n        # 添加统计信息\n        corr = df[feature].corr(df['y'])\n        ax.text(\n            0.05, 0.92, \n            f'Pearson R = {corr:.2f}',\n            transform=ax.transAxes,\n            fontsize=12,\n            bbox=dict(facecolor='white', alpha=0.8)\n        )\n        \n        # 设置坐标标签\n        ax.set_xlabel(feature.upper(), fontsize=14)\n        ax.set_ylabel('Target Y', fontsize=14)\n        ax.set_title(f'{feature} vs Y', fontsize=16)\n\n    # 调整整体标题（可选）\n    fig.suptitle('Feature-Target Relationships', fontsize=18, y=1.02)\n    plt.show()\n    \nshow_scatter(df)\n\n\n\n\n\n\n\n\n小批量读取数据集\n\nimport random\nimport torch\n\ndef data_iter(batch_size, features, labels):\n    if len(features) != len(labels):\n        print(\"len(features) != len(labels)\")\n        raise IndexError\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)\n    for i in range(0, num_examples, batch_size):\n        batch_indices = torch.tensor(\n            indices[i:min(i+batch_size, num_examples)]\n        )\n        yield features[batch_indices], labels[batch_indices]\n\n\n\n\n初始化模型参数w ,b\n\n# w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) # 长度为2的列向量\nw = torch.zeros((2, 1), requires_grad=True)\nb = torch.zeros(1, requires_grad=True)\n\n定义线性回归模型\n\ndef linreg(X, w, b):\n    return torch.matmul(X, w) + b\n\n\n\n\n\\mathrm{\\ell}(X, y, w, b)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\hat{y_i})^2}\n\ndef squared_loss(y_hat, y):\n    if len(y) != len(y_hat):\n        print(\"len(y) != len(y_hat)\")\n        raise ValueError\n    return ((y_hat - y.reshape(y_hat.shape)) ** 2).mean() / 2\n\n\n\n\n\ndef sgd(params, lr):\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad\n            param.grad.zero_()\n\n\n\n\n\nbatch_size = 10\nlr = 0.03\nnum_epochs = 3\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epochs):\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y)\n        l.backward()\n        sgd([w, b], lr)\n    with torch.no_grad():\n        train_l = loss(net(features, w, b), labels)\n        print(f'epoch {epoch+1}, loss {float(train_l.mean()):f}')\n\nepoch 1, loss 0.029633\nepoch 2, loss 0.000117\nepoch 3, loss 0.000049\n\n\n\nprint(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\nprint(f'b的估计误差: {true_b - b}')\n\nw的估计误差: tensor([ 3.9816e-05, -1.4021e-03], grad_fn=&lt;SubBackward0&gt;)\nb的估计误差: tensor([0.0001], grad_fn=&lt;RsubBackward1&gt;)"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day1/index.html#总结",
    "href": "posts/introduction-to-deep-learning-day1/index.html#总结",
    "title": "深度学习入门(day1) - 线性回归",
    "section": "",
    "text": "学习了基础的线性回归模型，当加入更多参数时，根据参数的取值范围和相关性，线性回归无法解决问题，需进一步了解数据预处理以及梯度下降算法。"
  }
]