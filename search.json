[
  {
    "objectID": "posts/introduction-to-deep-learning-day2/index.html",
    "href": "posts/introduction-to-deep-learning-day2/index.html",
    "title": "深度学习入门(day2) - softmax回归",
    "section": "",
    "text": "假设输入是一个 2\\times2 的灰度图像。每个图象对于四个特征 x_1, x_2, x_3, x_4。此外，假设每个图像都属于“猫”，“狗”，“鸡”中的一个。使用独热编码表示类别：(1, 0, 0)对应猫、(0, 1, 0)对应狗、(0, 0, 1)对应鸡： y \\in \\{ (1, 0, 0), (0, 1, 0), (0, 0, 1)\\} \n\n\n\n首先未每个输入计算三个未规范化的预测：o_1, o_2 \\text{和} o_3 \n\\begin{aligned}\no_1 &= x_1w_{11} + x_2w_{12} + x_3w_{13} + x_4w_{14} + b \\\\\no_2 &= x_1w_{21} + x_2w_{22} + x_3w_{23} + x_4w_{24} + b \\\\\no_3 &= x_1w_{31} + x_2w_{32} + x_3w_{33} + x_4w_{34} + b\n\\end{aligned}\n\n用神经网络图来描述计算过程\n\n\n\n将权重放到 3\\times4 的矩阵中，以上公式可以表示为：o = Wx + b。\n\n\n\n通过softmax运算可以将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。我们首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式： \n\\hat{y} = \\text{softmax}(o) \\quad 其中 \\quad \\hat{y_i} = \\frac{\\exp{(o_j)}}{\\sum_{k}\\exp{(o_k)}}\n\n\n\n\n使用交叉熵来衡量预测概率 \\hat{y} 与 y 的区别，公式可以表示为： \nL = -\\sum_{i=1}^{n} y_i \\log(\\hat{y_i})\n\n其中 \\hat{y_i} = \\frac{e^{o_i}}{\\sum_{j=1}^n e^{o_j}} 是softmax函数的输出。\n\n\n\n设 z_k 是最后一层的输入，求 \\frac{\\partial L}{\\partial o_k} ：\n1. 展开交叉熵损失： \nL = -\\sum_{i=1}^n y_i \\log\\left(\\frac{e^{o_i}}{\\sum_{j=1}^n e^{o_j}}\\right)\n\n2. 化简对数项： \nL = -\\sum_{i=1}^n y_i \\left(o_i - \\log\\left(\\sum_{j=1}^n e^{o_j}\\right)\\right)\n\n3. 分离求和项： \nL = -\\sum_{i=1}^n y_i o_i + \\log\\left(\\sum_{j=1}^n e^{o_j}\\right) \\cdot \\underbrace{\\sum_{i=1}^n y_i}_{=1}\n\n4. 对 z_k 求导数： \n\\frac{\\partial L}{\\partial o_k} = -y_k + \\frac{e^{o_k}}{\\sum_{j=1}^n e^{o_j}} = \\text{softmax}(o_k) - y_k\n\n—\n#### 最终梯度表达式\n\n\\boxed{\n\\frac{\\partial L}{\\partial o_k} =\n\\begin{cases}\n\\text{softmax}(o_k) - 1 & \\text{如果 } y_k = 1 \\\\\n\\text{softmax}(o_k) & \\text{其他情况}\n\\end{cases}\n}\n\n\n注：当且仅当 y_k 是真实类别时导数为 \\text{softmax}(o_k)-1，其他情况导数为 \\text{softmax}(o_k)。即真实类别时梯度为负，其他情况为正，优化是往正确概率的方向调整。\n\n\n\n\n\n\n\n\n使用内置的Fashion-MNIST数据集\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'  # 在 Jupyter 中设置矢量图显示\nimport torchvision\nfrom torchvision import transforms\nimport torch\nfrom torch.utils import data\n\n定义加载mnist数据集函数\n\ndef get_dataloader_workers():\n    return 4\n\ndef load_data_fashion_mnist(batch_size, resize=None):\n    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n    trans = [transforms.ToTensor()]\n    if resize:\n        trans.insert(0, transforms.Resize(resize))\n    trans = transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=True, transform=trans, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=False, transform=trans, download=True)\n    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n                            num_workers=get_dataloader_workers()),\n            data.DataLoader(mnist_test, batch_size, shuffle=False,\n                            num_workers=get_dataloader_workers()))\n\n\nbatch_size = 256\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size)\n\n初始化模型参数，由于要将输入的图片展开为一维，W 矩阵的形状为 784\\times10\n\nnum_inputs = 784\nnum_outputs = 10\n\nW = torch.normal(0, 0.1, size=(num_inputs, num_outputs), requires_grad=True)\nb = torch.zeros(num_outputs, requires_grad=True)\n\n\n\n\n\ndef softmax(X):\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepdim=True) # 对行求和，并保留维度，partition矩阵的每一个元素都是行和。\n    return X_exp / partition\n\n上述代码将X的矩阵每个元素变成元素非负，且行和为1的矩阵。\n\nX = torch.normal(0, 1, (2, 5))\nX_prob = softmax(X)\nX_prob, X_prob.sum(1)\n\n(tensor([[0.2113, 0.4260, 0.1268, 0.0907, 0.1452],\n         [0.1244, 0.6719, 0.1160, 0.0567, 0.0311]]),\n tensor([1.0000, 1.0000]))\n\n\n\n注：矩阵中非常大或者非常小的元素可能会造成数值上溢或下溢，代码并没有措施防止这点。\n\n\n\n\n模型为线性回归与softmax运算的组合\n\ndef net(X):\n    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)\n\n\n\n\n使用花式索引，以y作为y_hat中概率的索引，快速选择 \\hat{y_i}\n\ny = torch.tensor([0, 2, 1])\ny_hat = torch.tensor([[0.6, 0.3, 0.1], [0.3, 0.2, 0.5], [0.3, 0.4, 0.3]])\ny_hat[range(len(y_hat)), y]\n\ntensor([0.6000, 0.5000, 0.4000])\n\n\n一行代码就可以实现交叉熵损失函数\n\ndef cross_entropy(y_hat, y):\n    return -torch.log(y_hat[range(len(y_hat)), y]) # 化简后的交叉熵形式\n\ncross_entropy(y_hat, y)\n\ntensor([0.5108, 0.6931, 0.9163])\n\n\n\n\n\n用正确预测数量与总预测数量之比评估准确率，\\hat{y} 是一个存储概率的矩阵，利用torch.argmax()方法找出最大概率值的列号，与 y 进行对比。\n\ndef accuracy(y_hat, y):\n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1: # y_hat 为矩阵\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y\n    return float(cmp.type(y.dtype).sum())\n\n准确率为\n\naccuracy(y_hat, y) / len(y)\n\n1.0\n\n\n对于任意的迭代器data_iter，我们可以评估在任意模型net的精度。\n\n# 先定义一个累加器工具\nclass Accumulator:\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n # 定义长度为n的列表\n    \n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)] # 列表与输入值累加\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx] # 重写[]方法\n\ndef evaluate_accuracy(net, data_iter):\n    \"\"\"计算在指定数据集上模型的精度\"\"\"\n    metric = Accumulator(2) # [\"正确预测数\", \"预测总数\"]\n    with torch.no_grad():\n        for X, y in data_iter:\n            metric.add(accuracy(net(X), y), y.numel())\n    return metric[0] / metric[1]\n\nevaluate_accuracy(net, test_iter)\n\n0.0703\n\n\n\n\n\n首先，定义一个函数来训练一个迭代周期，updater是更新模型参数的常用函数，这里使用梯度下降算法。\n\ndef sgd(params, lr, batch_size):\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad / batch_size # 除以批量大小等价于损失函数中求批量的均值\n            param.grad.zero_()\n\nlr = 0.1\n\ndef updater(batch_size):\n    return sgd([W, b], lr, batch_size)\n\ndef train_epoch(net, train_iter, loss, updater):\n    \"\"\"训练模型一个迭代周期\"\"\"\n    metric = Accumulator(3) # 训练损失总和、训练准确度总和、样本数\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        l.sum().backward()\n        updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练精度\n    return metric[0] / metric[2], metric[1] / metric[2]\n    \n\n训练之前，定义一个在动画中绘制数据的实用程序类Animator，它可以可视化训练过程。\n\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\nclass Animator:\n    \"\"\"在动画中绘制数据\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        \n        # 创建图形和坐标轴\n        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes]  # 统一为列表格式方便后续操作\n            \n        # 坐标轴配置函数（原生替代d2l.set_axes）\n        def config_axes(ax):\n            ax.set_xlabel(xlabel)\n            ax.set_ylabel(ylabel)\n            if xlim: ax.set_xlim(xlim)\n            if ylim: ax.set_ylim(ylim)\n            ax.set_xscale(xscale)\n            ax.set_yscale(yscale)\n            if legend: ax.legend(legend)\n            ax.grid()\n            \n        self.config_axes = lambda: config_axes(self.axes[0])  # 绑定配置函数\n        \n        # 初始化数据容器\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        # 处理单值输入（兼容标量输入）\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        \n        # 处理x输入（兼容标量输入）\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n  # 所有y序列共享相同x值\n            \n        # 初始化数据存储列表\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n            \n        # 添加数据点\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        \n        # 清除当前图形并重绘\n        self.axes[0].cla()\n        for x_data, y_data, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x_data, y_data, fmt)\n            \n        # 配置坐标轴\n        self.config_axes()\n        \n        # 动态显示图形（需在Jupyter中运行）\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n实现一个训练函数，该训练函数将会运行多个迭代周期（由num_epochs指定）。在每个迭代周期结束时，利用test_iter访问到的测试数据集对模型进行评估。最后利用Animator类来可视化训练进度。\n\ndef train(net, train_iter, test_iter, loss, num_epochs, updater):\n    \"\"\"训练模型\"\"\"\n    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n                        legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        train_metrics = train_epoch(net, train_iter, loss, updater)\n        test_acc = evaluate_accuracy(net, test_iter)\n        animator.add(epoch + 1, train_metrics + (test_acc,))\n        \n    train_loss, train_acc = train_metrics\n    assert train_loss &lt; 0.5, train_loss\n    assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc\n    assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc\n\n训练10个迭代周期，学习率0.1\n\nnum_epoch = 10\ntrain(net, train_iter, test_iter, cross_entropy, num_epoch, updater)\n\n\n\n\n\n\n\n\n\n\n\n训练完成后，使用模型对图片进行分类预测，比较实际标签和预测。\n\ndef get_fashion_mnist_labels(labels):\n    \"\"\"返回Fashion-MNIST数据集的文本标签\"\"\"\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]\n\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n    \"\"\"绘制图像列表\"\"\"\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n    axes = axes.flatten()\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        ax.imshow(img.numpy())\n        ax.axes.get_xaxis().set_visible(False)\n        ax.axes.get_yaxis().set_visible(False)\n        if titles:\n            ax.set_title(titles[i])\n    return axes\n\ndef predict(net, test_iter, n=6):\n    \"\"\"预测标签\"\"\"\n    # for X, y in test_iter:\n    #     break\n    X, y = next(iter(test_iter))\n    trues = get_fashion_mnist_labels(y)\n    preds = get_fashion_mnist_labels(net(X).argmax(axis=1))\n    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n    show_images(\n        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n]\n    )\n\npredict(net, test_iter)\n\n\n\n\n\n\n\n\n\n\n\n\n训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day2/index.html#softmax回归的数学原理",
    "href": "posts/introduction-to-deep-learning-day2/index.html#softmax回归的数学原理",
    "title": "深度学习入门(day2) - softmax回归",
    "section": "",
    "text": "假设输入是一个 2\\times2 的灰度图像。每个图象对于四个特征 x_1, x_2, x_3, x_4。此外，假设每个图像都属于“猫”，“狗”，“鸡”中的一个。使用独热编码表示类别：(1, 0, 0)对应猫、(0, 1, 0)对应狗、(0, 0, 1)对应鸡： y \\in \\{ (1, 0, 0), (0, 1, 0), (0, 0, 1)\\} \n\n\n\n首先未每个输入计算三个未规范化的预测：o_1, o_2 \\text{和} o_3 \n\\begin{aligned}\no_1 &= x_1w_{11} + x_2w_{12} + x_3w_{13} + x_4w_{14} + b \\\\\no_2 &= x_1w_{21} + x_2w_{22} + x_3w_{23} + x_4w_{24} + b \\\\\no_3 &= x_1w_{31} + x_2w_{32} + x_3w_{33} + x_4w_{34} + b\n\\end{aligned}\n\n用神经网络图来描述计算过程\n\n\n\n将权重放到 3\\times4 的矩阵中，以上公式可以表示为：o = Wx + b。\n\n\n\n通过softmax运算可以将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。我们首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式： \n\\hat{y} = \\text{softmax}(o) \\quad 其中 \\quad \\hat{y_i} = \\frac{\\exp{(o_j)}}{\\sum_{k}\\exp{(o_k)}}\n\n\n\n\n使用交叉熵来衡量预测概率 \\hat{y} 与 y 的区别，公式可以表示为： \nL = -\\sum_{i=1}^{n} y_i \\log(\\hat{y_i})\n\n其中 \\hat{y_i} = \\frac{e^{o_i}}{\\sum_{j=1}^n e^{o_j}} 是softmax函数的输出。\n\n\n\n设 z_k 是最后一层的输入，求 \\frac{\\partial L}{\\partial o_k} ：\n1. 展开交叉熵损失： \nL = -\\sum_{i=1}^n y_i \\log\\left(\\frac{e^{o_i}}{\\sum_{j=1}^n e^{o_j}}\\right)\n\n2. 化简对数项： \nL = -\\sum_{i=1}^n y_i \\left(o_i - \\log\\left(\\sum_{j=1}^n e^{o_j}\\right)\\right)\n\n3. 分离求和项： \nL = -\\sum_{i=1}^n y_i o_i + \\log\\left(\\sum_{j=1}^n e^{o_j}\\right) \\cdot \\underbrace{\\sum_{i=1}^n y_i}_{=1}\n\n4. 对 z_k 求导数： \n\\frac{\\partial L}{\\partial o_k} = -y_k + \\frac{e^{o_k}}{\\sum_{j=1}^n e^{o_j}} = \\text{softmax}(o_k) - y_k\n\n—\n#### 最终梯度表达式\n\n\\boxed{\n\\frac{\\partial L}{\\partial o_k} =\n\\begin{cases}\n\\text{softmax}(o_k) - 1 & \\text{如果 } y_k = 1 \\\\\n\\text{softmax}(o_k) & \\text{其他情况}\n\\end{cases}\n}\n\n\n注：当且仅当 y_k 是真实类别时导数为 \\text{softmax}(o_k)-1，其他情况导数为 \\text{softmax}(o_k)。即真实类别时梯度为负，其他情况为正，优化是往正确概率的方向调整。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day2/index.html#softmax回归的代码实现",
    "href": "posts/introduction-to-deep-learning-day2/index.html#softmax回归的代码实现",
    "title": "深度学习入门(day2) - softmax回归",
    "section": "",
    "text": "使用内置的Fashion-MNIST数据集\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'  # 在 Jupyter 中设置矢量图显示\nimport torchvision\nfrom torchvision import transforms\nimport torch\nfrom torch.utils import data\n\n定义加载mnist数据集函数\n\ndef get_dataloader_workers():\n    return 4\n\ndef load_data_fashion_mnist(batch_size, resize=None):\n    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n    trans = [transforms.ToTensor()]\n    if resize:\n        trans.insert(0, transforms.Resize(resize))\n    trans = transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=True, transform=trans, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=False, transform=trans, download=True)\n    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n                            num_workers=get_dataloader_workers()),\n            data.DataLoader(mnist_test, batch_size, shuffle=False,\n                            num_workers=get_dataloader_workers()))\n\n\nbatch_size = 256\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size)\n\n初始化模型参数，由于要将输入的图片展开为一维，W 矩阵的形状为 784\\times10\n\nnum_inputs = 784\nnum_outputs = 10\n\nW = torch.normal(0, 0.1, size=(num_inputs, num_outputs), requires_grad=True)\nb = torch.zeros(num_outputs, requires_grad=True)\n\n\n\n\n\ndef softmax(X):\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepdim=True) # 对行求和，并保留维度，partition矩阵的每一个元素都是行和。\n    return X_exp / partition\n\n上述代码将X的矩阵每个元素变成元素非负，且行和为1的矩阵。\n\nX = torch.normal(0, 1, (2, 5))\nX_prob = softmax(X)\nX_prob, X_prob.sum(1)\n\n(tensor([[0.2113, 0.4260, 0.1268, 0.0907, 0.1452],\n         [0.1244, 0.6719, 0.1160, 0.0567, 0.0311]]),\n tensor([1.0000, 1.0000]))\n\n\n\n注：矩阵中非常大或者非常小的元素可能会造成数值上溢或下溢，代码并没有措施防止这点。\n\n\n\n\n模型为线性回归与softmax运算的组合\n\ndef net(X):\n    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)\n\n\n\n\n使用花式索引，以y作为y_hat中概率的索引，快速选择 \\hat{y_i}\n\ny = torch.tensor([0, 2, 1])\ny_hat = torch.tensor([[0.6, 0.3, 0.1], [0.3, 0.2, 0.5], [0.3, 0.4, 0.3]])\ny_hat[range(len(y_hat)), y]\n\ntensor([0.6000, 0.5000, 0.4000])\n\n\n一行代码就可以实现交叉熵损失函数\n\ndef cross_entropy(y_hat, y):\n    return -torch.log(y_hat[range(len(y_hat)), y]) # 化简后的交叉熵形式\n\ncross_entropy(y_hat, y)\n\ntensor([0.5108, 0.6931, 0.9163])\n\n\n\n\n\n用正确预测数量与总预测数量之比评估准确率，\\hat{y} 是一个存储概率的矩阵，利用torch.argmax()方法找出最大概率值的列号，与 y 进行对比。\n\ndef accuracy(y_hat, y):\n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1: # y_hat 为矩阵\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y\n    return float(cmp.type(y.dtype).sum())\n\n准确率为\n\naccuracy(y_hat, y) / len(y)\n\n1.0\n\n\n对于任意的迭代器data_iter，我们可以评估在任意模型net的精度。\n\n# 先定义一个累加器工具\nclass Accumulator:\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n # 定义长度为n的列表\n    \n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)] # 列表与输入值累加\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx] # 重写[]方法\n\ndef evaluate_accuracy(net, data_iter):\n    \"\"\"计算在指定数据集上模型的精度\"\"\"\n    metric = Accumulator(2) # [\"正确预测数\", \"预测总数\"]\n    with torch.no_grad():\n        for X, y in data_iter:\n            metric.add(accuracy(net(X), y), y.numel())\n    return metric[0] / metric[1]\n\nevaluate_accuracy(net, test_iter)\n\n0.0703\n\n\n\n\n\n首先，定义一个函数来训练一个迭代周期，updater是更新模型参数的常用函数，这里使用梯度下降算法。\n\ndef sgd(params, lr, batch_size):\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad / batch_size # 除以批量大小等价于损失函数中求批量的均值\n            param.grad.zero_()\n\nlr = 0.1\n\ndef updater(batch_size):\n    return sgd([W, b], lr, batch_size)\n\ndef train_epoch(net, train_iter, loss, updater):\n    \"\"\"训练模型一个迭代周期\"\"\"\n    metric = Accumulator(3) # 训练损失总和、训练准确度总和、样本数\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        l.sum().backward()\n        updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练精度\n    return metric[0] / metric[2], metric[1] / metric[2]\n    \n\n训练之前，定义一个在动画中绘制数据的实用程序类Animator，它可以可视化训练过程。\n\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\nclass Animator:\n    \"\"\"在动画中绘制数据\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        \n        # 创建图形和坐标轴\n        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes]  # 统一为列表格式方便后续操作\n            \n        # 坐标轴配置函数（原生替代d2l.set_axes）\n        def config_axes(ax):\n            ax.set_xlabel(xlabel)\n            ax.set_ylabel(ylabel)\n            if xlim: ax.set_xlim(xlim)\n            if ylim: ax.set_ylim(ylim)\n            ax.set_xscale(xscale)\n            ax.set_yscale(yscale)\n            if legend: ax.legend(legend)\n            ax.grid()\n            \n        self.config_axes = lambda: config_axes(self.axes[0])  # 绑定配置函数\n        \n        # 初始化数据容器\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        # 处理单值输入（兼容标量输入）\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        \n        # 处理x输入（兼容标量输入）\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n  # 所有y序列共享相同x值\n            \n        # 初始化数据存储列表\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n            \n        # 添加数据点\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        \n        # 清除当前图形并重绘\n        self.axes[0].cla()\n        for x_data, y_data, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x_data, y_data, fmt)\n            \n        # 配置坐标轴\n        self.config_axes()\n        \n        # 动态显示图形（需在Jupyter中运行）\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n实现一个训练函数，该训练函数将会运行多个迭代周期（由num_epochs指定）。在每个迭代周期结束时，利用test_iter访问到的测试数据集对模型进行评估。最后利用Animator类来可视化训练进度。\n\ndef train(net, train_iter, test_iter, loss, num_epochs, updater):\n    \"\"\"训练模型\"\"\"\n    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n                        legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        train_metrics = train_epoch(net, train_iter, loss, updater)\n        test_acc = evaluate_accuracy(net, test_iter)\n        animator.add(epoch + 1, train_metrics + (test_acc,))\n        \n    train_loss, train_acc = train_metrics\n    assert train_loss &lt; 0.5, train_loss\n    assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc\n    assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc\n\n训练10个迭代周期，学习率0.1\n\nnum_epoch = 10\ntrain(net, train_iter, test_iter, cross_entropy, num_epoch, updater)\n\n\n\n\n\n\n\n\n\n\n\n训练完成后，使用模型对图片进行分类预测，比较实际标签和预测。\n\ndef get_fashion_mnist_labels(labels):\n    \"\"\"返回Fashion-MNIST数据集的文本标签\"\"\"\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]\n\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n    \"\"\"绘制图像列表\"\"\"\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n    axes = axes.flatten()\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        ax.imshow(img.numpy())\n        ax.axes.get_xaxis().set_visible(False)\n        ax.axes.get_yaxis().set_visible(False)\n        if titles:\n            ax.set_title(titles[i])\n    return axes\n\ndef predict(net, test_iter, n=6):\n    \"\"\"预测标签\"\"\"\n    # for X, y in test_iter:\n    #     break\n    X, y = next(iter(test_iter))\n    trues = get_fashion_mnist_labels(y)\n    preds = get_fashion_mnist_labels(net(X).argmax(axis=1))\n    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n    show_images(\n        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n]\n    )\n\npredict(net, test_iter)"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day2/index.html#总结",
    "href": "posts/introduction-to-deep-learning-day2/index.html#总结",
    "title": "深度学习入门(day2) - softmax回归",
    "section": "",
    "text": "训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day4/index.html",
    "href": "posts/introduction-to-deep-learning-day4/index.html",
    "title": "深度学习入门(day4) - 感知机",
    "section": "",
    "text": "感知机接受多个信号，输出一个信号（1或0）。也可以称为人工神经元，以下是一个接收两个信号的感知机的例子。x_1 、 x_2 是输入信号，y 是输出信号，w_1、w_2 是权重。输入信号传入神经元的时候会被分别乘以权重（x_1w_1 、 x_2w_2）。只有传过来信号的总和大于某个阈值 \\theta 时，神经元才会被激活。\n\n\n\n两个输入的感知机\n\n\n用数学公式表示就是\n\ny=\\left\\{\n\\begin{array}{ll}\n0 & (w_1x_1 + w_2x_2 \\leq \\theta) \\\\\n1 & (w_1x_1 + w_2x_2 &gt; \\theta)\n\\end{array}\n\\right.\n\n感知机多个输入都有各自固有的权重，权重越大，对应该权重的信号的重要性就越高。\n\n\n\n\n\n接下来用感知机来表示逻辑电路，与门只有在两个输入都为1的时候才输出1，其他时候输出0\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0\n0\n\n\n0\n1\n0\n\n\n1\n0\n0\n\n\n1\n1\n1\n\n\n\n当 (w_1, w_2, \\theta)=(0.5, 0.5, 0.7) 时，设定这样的参数后，感知机满足与门的条件。当然，满足条件的参数选择方法可以有无数种。\n用python代码表示：\n\ndef AND(x1, x2):\n    w1, w2, theta = 0.5, 0.5, 0.7\n    tmp = x1 * w1 + x2 * w2\n    if tmp &lt;= theta:\n        return 0\n    else:\n        return 1\n\ninput = [(0, 0), (0, 1), (1, 0), (1, 1)]\n\nfor i in input:\n    print(f'输入：{i[0]} {i[1]} 输出：{AND(*i)}')\n\n输入：0 0 输出：0\n输入：0 1 输出：0\n输入：1 0 输出：0\n输入：1 1 输出：1\n\n\n\n\n\n与非门就是颠倒了与门的输出\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0\n1\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\n可以用 (w_1, w_2, \\theta)=(-0.5, -0.5, -0.7) 来实现与非门\n要表示或门\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n1\n\n\n\n可以设定 (w_1, w_2, \\theta)=(0.5, 0.5, 0.4)。\n接下来用另一种实现来表示感知机的行为：\n\ny=\\left\\{\n\\begin{array}{ll}\n0 & (b + w_1x_1 + w_2x_2 \\leq 0) \\\\\n1 & (b + w_1x_1 + w_2x_2 &gt; 0)\n\\end{array}\n\\right.\n\n至此，与门、与非门、或门可以用相同的构造的感知机表示，只有权重和偏置的值不同。\n\nimport numpy as np\n\ndef logic_gate(x1, x2, logic_gate):\n    x = np.array([x1, x2])\n    w = np.array(logic_gate[0:2])\n    b = logic_gate[2]\n    tmp = w @ x + b\n    if tmp &lt;= 0:\n        return 0\n    else:\n        return 1\n\nAND = (0.5, 0.5, -0.7)\nNAND = (-0.5, -0.5, 0.7)\nOR = (0.5, 0.5, -0.2)\n\n\ninput = [(0, 0), (0, 1), (1, 0), (1, 1)]\n\nfor i in input:\n    print(f'输入：{i[0]}    {i[1]} \\\n        与门输出：{logic_gate(*i, AND)} \\\n        与非门输出：{logic_gate(*i, NAND)} \\\n        或门输出：{logic_gate(*i, OR)}')\n\n输入：0    0         与门输出：0         与非门输出：1         或门输出：0\n输入：0    1         与门输出：0         与非门输出：1         或门输出：1\n输入：1    0         与门输出：0         与非门输出：1         或门输出：1\n输入：1    1         与门输出：1         与非门输出：0         或门输出：1\n\n\n\n\n\n\n\n\n如果要用感知机实现异或门\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\n在单层感知机中是无法实现异或门的，可以通过画图来解释其中原因。在下图中，直线下方表示输出为0的空间，上方表示输出为1的空间，四个点代表异或门的输入，要如何设定参数才能使直线 x_1w_1 + x_2w_2 + b 正好把四个点正确的分开？\n\n\n\n用坐标表示感知机\n\n\n无论如何用一条直线是做不到的。\n\n\n\n\n在电路中，通过与门、或门、非门之间的组合，是可以实现异或门的：\n\n\n\n多个门组成异或门\n\n\n真值表表示就是\n\n\n\nx1\nx2\ny1\ny2\ny\n\n\n\n\n0\n0\n1\n0\n0\n\n\n1\n0\n1\n1\n1\n\n\n0\n1\n1\n1\n1\n\n\n1\n1\n0\n1\n0\n\n\n\n下面试着用python实现异或门\n\ndef XOR(x1, x2):\n    s1 = logic_gate(x1, x2, NAND)\n    s2 = logic_gate(x1, x2, OR)\n    y = logic_gate(s1, s2, AND)\n    return y\n\ninput = [(0, 0), (0, 1), (1, 0), (1, 1)]\n\nfor i in input:\n    print(f'输入：{i[0]}    {i[1]} \\\n        异或门输出：{XOR(*i)}')\n\n输入：0    0         异或门输出：0\n输入：0    1         异或门输出：1\n输入：1    0         异或门输出：1\n输入：1    1         异或门输出：0\n\n\n与门、或门是单层感知机，异或门是2层感知机，叠加了多层的感知机也称为多层感知机。\n\n\n\n2层感知机\n\n\n\n\n\n理论上多层感知机可以像逻辑电路一样表示计算机。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day4/index.html#感知机是什么",
    "href": "posts/introduction-to-deep-learning-day4/index.html#感知机是什么",
    "title": "深度学习入门(day4) - 感知机",
    "section": "",
    "text": "感知机接受多个信号，输出一个信号（1或0）。也可以称为人工神经元，以下是一个接收两个信号的感知机的例子。x_1 、 x_2 是输入信号，y 是输出信号，w_1、w_2 是权重。输入信号传入神经元的时候会被分别乘以权重（x_1w_1 、 x_2w_2）。只有传过来信号的总和大于某个阈值 \\theta 时，神经元才会被激活。\n\n\n\n两个输入的感知机\n\n\n用数学公式表示就是\n\ny=\\left\\{\n\\begin{array}{ll}\n0 & (w_1x_1 + w_2x_2 \\leq \\theta) \\\\\n1 & (w_1x_1 + w_2x_2 &gt; \\theta)\n\\end{array}\n\\right.\n\n感知机多个输入都有各自固有的权重，权重越大，对应该权重的信号的重要性就越高。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day4/index.html#简单逻辑电路",
    "href": "posts/introduction-to-deep-learning-day4/index.html#简单逻辑电路",
    "title": "深度学习入门(day4) - 感知机",
    "section": "",
    "text": "接下来用感知机来表示逻辑电路，与门只有在两个输入都为1的时候才输出1，其他时候输出0\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0\n0\n\n\n0\n1\n0\n\n\n1\n0\n0\n\n\n1\n1\n1\n\n\n\n当 (w_1, w_2, \\theta)=(0.5, 0.5, 0.7) 时，设定这样的参数后，感知机满足与门的条件。当然，满足条件的参数选择方法可以有无数种。\n用python代码表示：\n\ndef AND(x1, x2):\n    w1, w2, theta = 0.5, 0.5, 0.7\n    tmp = x1 * w1 + x2 * w2\n    if tmp &lt;= theta:\n        return 0\n    else:\n        return 1\n\ninput = [(0, 0), (0, 1), (1, 0), (1, 1)]\n\nfor i in input:\n    print(f'输入：{i[0]} {i[1]} 输出：{AND(*i)}')\n\n输入：0 0 输出：0\n输入：0 1 输出：0\n输入：1 0 输出：0\n输入：1 1 输出：1\n\n\n\n\n\n与非门就是颠倒了与门的输出\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0\n1\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\n可以用 (w_1, w_2, \\theta)=(-0.5, -0.5, -0.7) 来实现与非门\n要表示或门\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n1\n\n\n\n可以设定 (w_1, w_2, \\theta)=(0.5, 0.5, 0.4)。\n接下来用另一种实现来表示感知机的行为：\n\ny=\\left\\{\n\\begin{array}{ll}\n0 & (b + w_1x_1 + w_2x_2 \\leq 0) \\\\\n1 & (b + w_1x_1 + w_2x_2 &gt; 0)\n\\end{array}\n\\right.\n\n至此，与门、与非门、或门可以用相同的构造的感知机表示，只有权重和偏置的值不同。\n\nimport numpy as np\n\ndef logic_gate(x1, x2, logic_gate):\n    x = np.array([x1, x2])\n    w = np.array(logic_gate[0:2])\n    b = logic_gate[2]\n    tmp = w @ x + b\n    if tmp &lt;= 0:\n        return 0\n    else:\n        return 1\n\nAND = (0.5, 0.5, -0.7)\nNAND = (-0.5, -0.5, 0.7)\nOR = (0.5, 0.5, -0.2)\n\n\ninput = [(0, 0), (0, 1), (1, 0), (1, 1)]\n\nfor i in input:\n    print(f'输入：{i[0]}    {i[1]} \\\n        与门输出：{logic_gate(*i, AND)} \\\n        与非门输出：{logic_gate(*i, NAND)} \\\n        或门输出：{logic_gate(*i, OR)}')\n\n输入：0    0         与门输出：0         与非门输出：1         或门输出：0\n输入：0    1         与门输出：0         与非门输出：1         或门输出：1\n输入：1    0         与门输出：0         与非门输出：1         或门输出：1\n输入：1    1         与门输出：1         与非门输出：0         或门输出：1"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day4/index.html#感知机的极限",
    "href": "posts/introduction-to-deep-learning-day4/index.html#感知机的极限",
    "title": "深度学习入门(day4) - 感知机",
    "section": "",
    "text": "如果要用感知机实现异或门\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\n在单层感知机中是无法实现异或门的，可以通过画图来解释其中原因。在下图中，直线下方表示输出为0的空间，上方表示输出为1的空间，四个点代表异或门的输入，要如何设定参数才能使直线 x_1w_1 + x_2w_2 + b 正好把四个点正确的分开？\n\n\n\n用坐标表示感知机\n\n\n无论如何用一条直线是做不到的。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day4/index.html#多层感知机",
    "href": "posts/introduction-to-deep-learning-day4/index.html#多层感知机",
    "title": "深度学习入门(day4) - 感知机",
    "section": "",
    "text": "在电路中，通过与门、或门、非门之间的组合，是可以实现异或门的：\n\n\n\n多个门组成异或门\n\n\n真值表表示就是\n\n\n\nx1\nx2\ny1\ny2\ny\n\n\n\n\n0\n0\n1\n0\n0\n\n\n1\n0\n1\n1\n1\n\n\n0\n1\n1\n1\n1\n\n\n1\n1\n0\n1\n0\n\n\n\n下面试着用python实现异或门\n\ndef XOR(x1, x2):\n    s1 = logic_gate(x1, x2, NAND)\n    s2 = logic_gate(x1, x2, OR)\n    y = logic_gate(s1, s2, AND)\n    return y\n\ninput = [(0, 0), (0, 1), (1, 0), (1, 1)]\n\nfor i in input:\n    print(f'输入：{i[0]}    {i[1]} \\\n        异或门输出：{XOR(*i)}')\n\n输入：0    0         异或门输出：0\n输入：0    1         异或门输出：1\n输入：1    0         异或门输出：1\n输入：1    1         异或门输出：0\n\n\n与门、或门是单层感知机，异或门是2层感知机，叠加了多层的感知机也称为多层感知机。\n\n\n\n2层感知机"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day4/index.html#总结",
    "href": "posts/introduction-to-deep-learning-day4/index.html#总结",
    "title": "深度学习入门(day4) - 感知机",
    "section": "",
    "text": "理论上多层感知机可以像逻辑电路一样表示计算机。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "notebook",
    "section": "",
    "text": "深度学习入门(day4) - 感知机\n\n\n感知机接受多个信号，输出一个信号（1或0）。也可以称为人工神经元。\n\n\n\n\n\nMay 24, 2025\n\n\nTUT\n\n\n\n\n\n\n\n\n\n\n\n\n深度学习入门(day3) - pytorch实现回归模型\n\n\n学习了线性回归和softmax回归之后，用pytroch简洁实现。\n\n\n\n\n\nMay 23, 2025\n\n\nTUT\n\n\n\n\n\n\n\n\n\n\n\n\n深度学习入门(day2) - softmax回归\n\n\n回归除了可以用于预测多少的问题，也可以用于解决分类问题。\n\n\n\n\n\nMay 22, 2025\n\n\nTUT\n\n\n\n\n\n\n\n\n\n\n\n\n深度学习入门(day1) - 线性回归\n\n\n通过了解线性回归的数学原理，用python代码从零开始实现简单的线性回归模型。\n\n\n\n\n\nMay 20, 2025\n\n\nTUT\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\n Back to top"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day1/index.html",
    "href": "posts/introduction-to-deep-learning-day1/index.html",
    "title": "深度学习入门(day1) - 线性回归",
    "section": "",
    "text": "线性回归基于几个简单的假设：首先，假设自变量 x 和因变量 y 之间的关系是线性的，即 y 可以表示为 x 中元素的加权和，这里通常允许包含观测值的一些噪声；其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。\n有一组数据 \\mathbf{X} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1n} \\\\\nx_{21} & x_{22} & \\cdots & x_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{d1} & x_{d2} & \\cdots & x_{dn}\n\\end{bmatrix} 和 y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} 要如何找到一条由 w = [w_1, w_2, w_3, \\dots, w_n] 和 b 确定的曲线 \\hat{y}=wx^T+b，使得 \\hat{y} 尽可能的接近 y?\n\n如何表示预测值 \\hat{y} 与真实值 y 的接近程度？\n使用均方损失 \\mathrm{\\ell}(X, y, w, b)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\hat{y_i})^2}=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\langle w, x_i \\rangle - b)^2}=\\frac{1}{2n}||y-(Xw+b)||^2 ，用 \\frac{1}{2} 抵消求导后的系数简化计算。\n如何根据损失更新 w 和 b？\n对于给定的 X, y，求出一组 w^*, b^* 使得 \\mathrm{\\ell}(X, y, w, b) 取得最小值：w^*, b^*=\\underset{w, b}{\\arg\\max}{\\mathrm{\\ell}(X, y, w, b)}。对于一些函数可以用偏导数为0解出极值点，但是大多数情况无解，所以要用到梯度下降算法：  w_{k+1} = w_{k} - \\alpha\\frac{\\partial \\ell}{\\partial w}   将偏差b加入到权重简化计算：X \\leftarrow \\begin{bmatrix} X \\\\ 1 \\end{bmatrix}，w \\leftarrow \\begin{bmatrix} w \\\\ b \\end{bmatrix}\n\n\n\n\n\n\n首先导入包\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nimport numpy as np\nimport pandas as pd\nimport torch\n\n生成训练数据集\n\n# 生成配置参数\ndef synthetic_data(w, b, num_examples):  #@save\n    \"\"\"生成y=Xw+b+噪声\"\"\"\n    X = torch.normal(0, 1, (num_examples, len(w)))\n    y = torch.matmul(X, w) + b\n    y += torch.normal(0, 0.01, y.shape)\n    return X, y.reshape((-1, 1))\n\ntrue_w = torch.tensor([2.4, -2.1])\ntrue_b = 4.1\nfeatures, labels = synthetic_data(true_w, true_b, 1000)\n\n可视化特征与目标值的关系\n\ndf = pd.DataFrame(features, columns=[f'x{i+1}' for i in range(len(true_w))])\ndf['y'] = labels\n\nimport matplotlib.pyplot as plt\n\ndef show_scatter(df):\n    features = df.columns[0:-1]  # 获取所有特征列名\n\n    # 创建组合画布（宽度增大适应横向布局）\n    fig, axes = plt.subplots(\n        nrows=1,  # 单行布局\n        ncols=len(features),  # 列数等于特征数量\n        figsize=(6 * len(features), 3.5),  # 动态宽度适配特征数量\n        dpi=100,\n        constrained_layout=True  # 自动调整子图间距\n    )\n\n    # 并行循环特征与对应的坐标轴\n    for ax, feature in zip(axes, features):\n        # 在指定坐标轴上绘制散点图\n        sc = ax.scatter(\n            df[feature], df['y'], \n            alpha=0.6,\n            edgecolor='w',\n            s=45, \n            c=df[feature],\n            cmap='viridis'\n        )\n        \n        # 添加颜色条（共享同一刻度）\n        if ax == axes[-1]:  # 只在最后一个坐标轴添加颜色条\n            plt.colorbar(sc, ax=axes, label=feature.upper())\n        \n        # 添加统计信息\n        corr = df[feature].corr(df['y'])\n        ax.text(\n            0.05, 0.92, \n            f'Pearson R = {corr:.2f}',\n            transform=ax.transAxes,\n            fontsize=12,\n            bbox=dict(facecolor='white', alpha=0.8)\n        )\n        \n        # 设置坐标标签\n        ax.set_xlabel(feature.upper(), fontsize=14)\n        ax.set_ylabel('Target Y', fontsize=14)\n        ax.set_title(f'{feature} vs Y', fontsize=16)\n\n    # 调整整体标题（可选）\n    fig.suptitle('Feature-Target Relationships', fontsize=18, y=1.02)\n    plt.show()\n    \nshow_scatter(df)\n\n\n\n\n\n\n\n\n小批量读取数据集\n\nimport random\nimport torch\n\ndef data_iter(batch_size, features, labels):\n    if len(features) != len(labels):\n        print(\"len(features) != len(labels)\")\n        raise IndexError\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)\n    for i in range(0, num_examples, batch_size):\n        batch_indices = torch.tensor(\n            indices[i:min(i+batch_size, num_examples)]\n        )\n        yield features[batch_indices], labels[batch_indices]\n\n\n\n\n初始化模型参数w ,b\n\n# w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) # 长度为2的列向量\nw = torch.zeros((2, 1), requires_grad=True)\nb = torch.zeros(1, requires_grad=True)\n\n定义线性回归模型\n\ndef linreg(X, w, b):\n    return torch.matmul(X, w) + b\n\n\n\n\n\\mathrm{\\ell}(X, y, w, b)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\hat{y_i})^2}\n\ndef squared_loss(y_hat, y):\n    if len(y) != len(y_hat):\n        print(\"len(y) != len(y_hat)\")\n        raise ValueError\n    return ((y_hat - y.reshape(y_hat.shape)) ** 2).mean() / 2\n\n\n\n\n\ndef sgd(params, lr):\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad\n            param.grad.zero_()\n\n\n\n\n\nbatch_size = 10\nlr = 0.03\nnum_epochs = 3\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epochs):\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y)\n        l.backward()\n        sgd([w, b], lr)\n    with torch.no_grad():\n        train_l = loss(net(features, w, b), labels)\n        print(f'epoch {epoch+1}, loss {float(train_l.mean()):f}')\n\nepoch 1, loss 0.029633\nepoch 2, loss 0.000117\nepoch 3, loss 0.000049\n\n\n\nprint(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\nprint(f'b的估计误差: {true_b - b}')\n\nw的估计误差: tensor([ 3.9816e-05, -1.4021e-03], grad_fn=&lt;SubBackward0&gt;)\nb的估计误差: tensor([0.0001], grad_fn=&lt;RsubBackward1&gt;)\n\n\n\n\n\n\n学习了基础的线性回归模型，当加入更多参数时，根据参数的取值范围和相关性，线性回归无法解决问题，需进一步了解数据预处理以及梯度下降算法。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day1/index.html#线性回归的数学原理",
    "href": "posts/introduction-to-deep-learning-day1/index.html#线性回归的数学原理",
    "title": "深度学习入门(day1) - 线性回归",
    "section": "",
    "text": "线性回归基于几个简单的假设：首先，假设自变量 x 和因变量 y 之间的关系是线性的，即 y 可以表示为 x 中元素的加权和，这里通常允许包含观测值的一些噪声；其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。\n有一组数据 \\mathbf{X} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\cdots & x_{1n} \\\\\nx_{21} & x_{22} & \\cdots & x_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{d1} & x_{d2} & \\cdots & x_{dn}\n\\end{bmatrix} 和 y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} 要如何找到一条由 w = [w_1, w_2, w_3, \\dots, w_n] 和 b 确定的曲线 \\hat{y}=wx^T+b，使得 \\hat{y} 尽可能的接近 y?\n\n如何表示预测值 \\hat{y} 与真实值 y 的接近程度？\n使用均方损失 \\mathrm{\\ell}(X, y, w, b)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\hat{y_i})^2}=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\langle w, x_i \\rangle - b)^2}=\\frac{1}{2n}||y-(Xw+b)||^2 ，用 \\frac{1}{2} 抵消求导后的系数简化计算。\n如何根据损失更新 w 和 b？\n对于给定的 X, y，求出一组 w^*, b^* 使得 \\mathrm{\\ell}(X, y, w, b) 取得最小值：w^*, b^*=\\underset{w, b}{\\arg\\max}{\\mathrm{\\ell}(X, y, w, b)}。对于一些函数可以用偏导数为0解出极值点，但是大多数情况无解，所以要用到梯度下降算法：  w_{k+1} = w_{k} - \\alpha\\frac{\\partial \\ell}{\\partial w}   将偏差b加入到权重简化计算：X \\leftarrow \\begin{bmatrix} X \\\\ 1 \\end{bmatrix}，w \\leftarrow \\begin{bmatrix} w \\\\ b \\end{bmatrix}"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day1/index.html#线性回归代码实现",
    "href": "posts/introduction-to-deep-learning-day1/index.html#线性回归代码实现",
    "title": "深度学习入门(day1) - 线性回归",
    "section": "",
    "text": "首先导入包\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nimport numpy as np\nimport pandas as pd\nimport torch\n\n生成训练数据集\n\n# 生成配置参数\ndef synthetic_data(w, b, num_examples):  #@save\n    \"\"\"生成y=Xw+b+噪声\"\"\"\n    X = torch.normal(0, 1, (num_examples, len(w)))\n    y = torch.matmul(X, w) + b\n    y += torch.normal(0, 0.01, y.shape)\n    return X, y.reshape((-1, 1))\n\ntrue_w = torch.tensor([2.4, -2.1])\ntrue_b = 4.1\nfeatures, labels = synthetic_data(true_w, true_b, 1000)\n\n可视化特征与目标值的关系\n\ndf = pd.DataFrame(features, columns=[f'x{i+1}' for i in range(len(true_w))])\ndf['y'] = labels\n\nimport matplotlib.pyplot as plt\n\ndef show_scatter(df):\n    features = df.columns[0:-1]  # 获取所有特征列名\n\n    # 创建组合画布（宽度增大适应横向布局）\n    fig, axes = plt.subplots(\n        nrows=1,  # 单行布局\n        ncols=len(features),  # 列数等于特征数量\n        figsize=(6 * len(features), 3.5),  # 动态宽度适配特征数量\n        dpi=100,\n        constrained_layout=True  # 自动调整子图间距\n    )\n\n    # 并行循环特征与对应的坐标轴\n    for ax, feature in zip(axes, features):\n        # 在指定坐标轴上绘制散点图\n        sc = ax.scatter(\n            df[feature], df['y'], \n            alpha=0.6,\n            edgecolor='w',\n            s=45, \n            c=df[feature],\n            cmap='viridis'\n        )\n        \n        # 添加颜色条（共享同一刻度）\n        if ax == axes[-1]:  # 只在最后一个坐标轴添加颜色条\n            plt.colorbar(sc, ax=axes, label=feature.upper())\n        \n        # 添加统计信息\n        corr = df[feature].corr(df['y'])\n        ax.text(\n            0.05, 0.92, \n            f'Pearson R = {corr:.2f}',\n            transform=ax.transAxes,\n            fontsize=12,\n            bbox=dict(facecolor='white', alpha=0.8)\n        )\n        \n        # 设置坐标标签\n        ax.set_xlabel(feature.upper(), fontsize=14)\n        ax.set_ylabel('Target Y', fontsize=14)\n        ax.set_title(f'{feature} vs Y', fontsize=16)\n\n    # 调整整体标题（可选）\n    fig.suptitle('Feature-Target Relationships', fontsize=18, y=1.02)\n    plt.show()\n    \nshow_scatter(df)\n\n\n\n\n\n\n\n\n小批量读取数据集\n\nimport random\nimport torch\n\ndef data_iter(batch_size, features, labels):\n    if len(features) != len(labels):\n        print(\"len(features) != len(labels)\")\n        raise IndexError\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)\n    for i in range(0, num_examples, batch_size):\n        batch_indices = torch.tensor(\n            indices[i:min(i+batch_size, num_examples)]\n        )\n        yield features[batch_indices], labels[batch_indices]\n\n\n\n\n初始化模型参数w ,b\n\n# w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) # 长度为2的列向量\nw = torch.zeros((2, 1), requires_grad=True)\nb = torch.zeros(1, requires_grad=True)\n\n定义线性回归模型\n\ndef linreg(X, w, b):\n    return torch.matmul(X, w) + b\n\n\n\n\n\\mathrm{\\ell}(X, y, w, b)=\\frac{1}{2}\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^{n}{(y_i-\\hat{y_i})^2}\n\ndef squared_loss(y_hat, y):\n    if len(y) != len(y_hat):\n        print(\"len(y) != len(y_hat)\")\n        raise ValueError\n    return ((y_hat - y.reshape(y_hat.shape)) ** 2).mean() / 2\n\n\n\n\n\ndef sgd(params, lr):\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad\n            param.grad.zero_()\n\n\n\n\n\nbatch_size = 10\nlr = 0.03\nnum_epochs = 3\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epochs):\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y)\n        l.backward()\n        sgd([w, b], lr)\n    with torch.no_grad():\n        train_l = loss(net(features, w, b), labels)\n        print(f'epoch {epoch+1}, loss {float(train_l.mean()):f}')\n\nepoch 1, loss 0.029633\nepoch 2, loss 0.000117\nepoch 3, loss 0.000049\n\n\n\nprint(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\nprint(f'b的估计误差: {true_b - b}')\n\nw的估计误差: tensor([ 3.9816e-05, -1.4021e-03], grad_fn=&lt;SubBackward0&gt;)\nb的估计误差: tensor([0.0001], grad_fn=&lt;RsubBackward1&gt;)"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day1/index.html#总结",
    "href": "posts/introduction-to-deep-learning-day1/index.html#总结",
    "title": "深度学习入门(day1) - 线性回归",
    "section": "",
    "text": "学习了基础的线性回归模型，当加入更多参数时，根据参数的取值范围和相关性，线性回归无法解决问题，需进一步了解数据预处理以及梯度下降算法。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day3/index.html",
    "href": "posts/introduction-to-deep-learning-day3/index.html",
    "title": "深度学习入门(day3) - pytorch实现回归模型",
    "section": "",
    "text": "使用pytroch简洁的实现线性回归模型 ### 生成数据集 首先生成人工数据集\n\nimport numpy as np\nimport torch\nfrom torch.utils import data\n\ndef synthetic_data(w, b, num_examples):  #@save\n    \"\"\"生成y=Xw+b+噪声\"\"\"\n    X = torch.normal(0, 1, (num_examples, len(w)))\n    y = torch.matmul(X, w) + b\n    y += torch.normal(0, 0.01, y.shape)\n    return X, y.reshape((-1, 1))\n\ntrue_w = torch.tensor([2, -3.4])\ntrue_b = torch.tensor(4.2)\n\nfeatures, labels = synthetic_data(true_w, true_b, 1000)\n\n\n\n调用torch.utils模块中的DataLoader方法读取数据集，它可以通过设定shuffle自动打乱顺序，同时返回批量为batch_size的可迭代对象。\n\ndef load_array(data_array, batch_size, is_train=True):\n    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n    dataset = data.TensorDataset(*data_array) # 传入的data_array是元组(X, y)，要先解包，它能将多个tensor对齐组成数据集。\n    return data.DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n\nbatch_size = 10\ndata_iter = load_array((features, labels), batch_size)\n\nnext(iter(data_iter))\n\n[tensor([[ 1.2391,  0.5222],\n         [ 0.6032, -0.7112],\n         [-0.3235, -0.7194],\n         [-0.8582,  0.7941],\n         [-0.3545,  0.7127],\n         [-0.1352, -2.4194],\n         [ 0.8341,  0.3817],\n         [-0.4976,  0.1100],\n         [ 0.1163, -1.0321],\n         [ 1.5996, -0.9930]]),\n tensor([[ 4.9098],\n         [ 7.8291],\n         [ 5.9892],\n         [-0.2285],\n         [ 1.0782],\n         [12.1557],\n         [ 4.5773],\n         [ 2.8291],\n         [ 7.9469],\n         [10.7816]])]\n\n\n\n\n\n首先定义一个模型变量net，它是一个Sequential类的实例。当给定输入数据时，它能将数据传入第一层，然后将第一次层的输出作为第二层的输入。。。\n\nfrom torch import nn\n\nnet = nn.Sequential(nn.Linear(2, 1)) # Linear方法接受两个参数，第一个参数指定输入特征的形状，第二个指定输出特征的形状。\n\n\n\n\n可以通过net[0]来选定第一个层，使用weight.data和bias.data访问参数，它会被初始化成随机值，可以使用normal_和fill_来重写参数。\n\nnet[0].weight.data.normal_(0, 0.01)\nnet[0].bias.data.fill_(0)\n\ntensor([0.])\n\n\n\n\n\npytorch中的均方损失是MSELoss类，它默认返回所有样本损失的平均值（由reduction指定mean、sum、none）。\n\nloss = nn.MSELoss()\n\n\n\n\n在pytorch的optim模块中有很多梯度下降算法的变种，这里使用SGD，它接收的params参数通过net.parameters()获得\n\ntrainer =torch.optim.SGD(net.parameters(), lr=0.03)\n\n\n\n对于每一个迭代周期里，通过迭代器不停的获取小批量的输入和相应的标签，对每一个小批量数据执行:\n\n通过调用net(X)生成预测并计算损失l（前向传播）。\n通过进行反向传播来计算梯度。\n通过调用优化器来更新模型参数。\n\n\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    for X, y in data_iter:\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        trainer.zero_grad() # 清空梯度\n        l.backward()\n        trainer.step() # 执行一步梯度下降，更新参数。\n    l = loss(net(features), labels) # 观察每一个迭代周期的训练效果\n    print(f'epoch {epoch+1}, loss {l:f}')\n\nw = net[0].weight.data\nprint('w的估计误差：', true_w - w.reshape(true_w.shape))\nb = net[0].bias.data\nprint('b的估计误差：', true_b - b)\n\nepoch 1, loss 0.000299\nepoch 2, loss 0.000102\nepoch 3, loss 0.000101\nw的估计误差： tensor([-8.7142e-04, -3.1710e-05])\nb的估计误差： tensor([0.0005])\n\n\n\n\n\n\n\n通过深度学习框架的高级API也能更方便地实现softmax回归模型。\n\nimport torch\nfrom torch import nn\nimport torchvision\nfrom torchvision import transforms\n\n\ndef get_dataloader_workers():\n    return 4\n\ndef load_data_fashion_mnist(batch_size, resize=None):\n    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n    trans = [transforms.ToTensor()]\n    if resize:\n        trans.insert(0, transforms.Resize(resize))\n    trans = transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=True, transform=trans, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=False, transform=trans, download=True)\n    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n                            num_workers=get_dataloader_workers()),\n            data.DataLoader(mnist_test, batch_size, shuffle=False,\n                            num_workers=get_dataloader_workers()))\n\nbatch_size = 256\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size)\n\n\n\nsoftmax回归的输出层是一个全连接层。因此，只需在Sequential中添加一个带有10个输出的全连接层。\n\n# PyTorch不会隐式地调整输入的形状。因此，\n# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状\n# 通常第 0 维是批大小（batch_size），默认跳过，从第 1 维开始展平。\nnet = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights)\n\nSequential(\n  (0): Flatten(start_dim=1, end_dim=-1)\n  (1): Linear(in_features=784, out_features=10, bias=True)\n)\n\n\n\n\n\n对于之前的softmax函数 \\hat{y_i}=\\frac{e^{o_j}}{\\sum_{k}{e^{o_k}}}，如果 o_k 中的一些数值非常大，那么 e^{o_k} 可能会大过数据类型的上限，即上溢。可能会使分母变成inf，导致结果为0、inf或nan的 \\hat{y_j}。\n解决技巧是：先从所有 o_k 中减去 \\max{(o_k)}。\n\n\\begin{aligned}\n\\hat{y_i} &= \\frac{e^{o_j-\\max{(o_k)}}e^{\\max{(o_k)}}}{\\sum_{k}e^{o_k-\\max{(o_k)}}e^{\\max{(o_k)}}} \\\\\n&= \\frac{e^{o_j-\\max{(o_k)}}}{\\sum_{k}e^{o_k-\\max{(o_k)}}}\n\\end{aligned}\n\n经过以上步骤，可能有一些 o_j - \\max{(o_k)} 具有较大的负值。由于精度受限，e^{o_j - \\max{(o_k)}} 将会有接近零的值，即下溢。这些值可能会四舍五入为零，使 \\hat{y_i} 的值为 -\\infty。\n将softmax和交叉熵结合在一起，可以避免计算指数函数： \n\\begin{aligned}\n\\log{(\\hat{y_i})} &= \\log{\\left( \\frac{e^{o_j-\\max{(o_k)}}}{\\sum_{k}e^{o_k-\\max{(o_k)}}} \\right)} \\\\\n&= \\log{(e^{(o_j - \\max{(o_k)})})} - \\log{\\left( \\sum_{k}e^{o_k-\\max{(o_k)}} \\right)} \\\\\n&= o_j - \\max{(o_k)} - \\log{\\left( \\sum_{k}e^{o_k-\\max{(o_k)}} \\right)}\n\\end{aligned}\n\n最后在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数\n\n\"\"\"自动对模型的原始输出（logits，未归一化的分数）进行 LogSoftmax 处理，无需手动添加 Softmax 层。\"\"\"\nloss = nn.CrossEntropyLoss(reduction='none') \n\n\n\n\n使用学习率为0.1的小批量随机梯度下降作为优化算法。\n\ntrainer = torch.optim.SGD(net.parameters(), lr=0.1)\n\n\n\n\n先定义绘图工具类\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'  # 在 Jupyter 中设置矢量图显示\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\ndef accuracy(y_hat, y):\n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1: # y_hat 为矩阵\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y\n    return float(cmp.type(y.dtype).sum())\n\n\n# 先定义一个累加器工具\nclass Accumulator:\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n # 定义长度为n的列表\n    \n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)] # 列表与输入值累加\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx] # 重写[]方法\n\n\ndef evaluate_accuracy(net, data_iter):\n    \"\"\"计算在指定数据集上模型的精度\"\"\"\n    if isinstance(net, torch.nn.Module):\n        net.eval()  # 将模型设置为评估模式\n    metric = Accumulator(2)  # 正确预测数、预测总数\n    with torch.no_grad():\n        for X, y in data_iter:\n            metric.add(accuracy(net(X), y), y.numel())\n    return metric[0] / metric[1]\n\nclass Animator:\n    \"\"\"在动画中绘制数据\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        \n        # 创建图形和坐标轴\n        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes]  # 统一为列表格式方便后续操作\n            \n        # 坐标轴配置函数（原生替代d2l.set_axes）\n        def config_axes(ax):\n            ax.set_xlabel(xlabel)\n            ax.set_ylabel(ylabel)\n            if xlim: ax.set_xlim(xlim)\n            if ylim: ax.set_ylim(ylim)\n            ax.set_xscale(xscale)\n            ax.set_yscale(yscale)\n            if legend: ax.legend(legend)\n            ax.grid()\n            \n        self.config_axes = lambda: config_axes(self.axes[0])  # 绑定配置函数\n        \n        # 初始化数据容器\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        # 处理单值输入（兼容标量输入）\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        \n        # 处理x输入（兼容标量输入）\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n  # 所有y序列共享相同x值\n            \n        # 初始化数据存储列表\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n            \n        # 添加数据点\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        \n        # 清除当前图形并重绘\n        self.axes[0].cla()\n        for x_data, y_data, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x_data, y_data, fmt)\n            \n        # 配置坐标轴\n        self.config_axes()\n        \n        # 动态显示图形（需在Jupyter中运行）\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n再定义训练函数\n\ndef train_epoch(net, train_iter, loss, updater):  #@save\n    \"\"\"训练模型一个迭代周期\"\"\"\n    # 将模型设置为训练模式\n    if isinstance(net, torch.nn.Module):\n        net.train()\n    # 训练损失总和、训练准确度总和、样本数\n    metric = Accumulator(3)\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        if isinstance(updater, torch.optim.Optimizer):\n            # 使用PyTorch内置的优化器和损失函数\n            updater.zero_grad()\n            l.mean().backward()\n            updater.step()\n        else:\n            # 使用定制的优化器和损失函数\n            l.sum().backward()\n            updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练精度\n    return metric[0] / metric[2], metric[1] / metric[2]\n\ndef train(net, train_iter, test_iter, loss, num_epochs, updater):\n    \"\"\"训练模型\"\"\"\n    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n                        legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        train_metrics = train_epoch(net, train_iter, loss, updater)\n        test_acc = evaluate_accuracy(net, test_iter)\n        animator.add(epoch + 1, train_metrics + (test_acc,))\n    train_loss, train_acc = train_metrics\n    assert train_loss &lt; 0.5, train_loss\n    assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc\n    assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc\n\n开始训练\n\nnum_epoch = 10\ntrain(net, train_iter, test_iter, loss, num_epoch, trainer)\n\n\n\n\n\n\n\n\n\n\n\n\n使用深度学习框架的内置API，不仅可以简洁高效的训练模型，还额外采取预防措施避免数值异常。"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day3/index.html#线性回归的简洁实现",
    "href": "posts/introduction-to-deep-learning-day3/index.html#线性回归的简洁实现",
    "title": "深度学习入门(day3) - pytorch实现回归模型",
    "section": "",
    "text": "使用pytroch简洁的实现线性回归模型 ### 生成数据集 首先生成人工数据集\n\nimport numpy as np\nimport torch\nfrom torch.utils import data\n\ndef synthetic_data(w, b, num_examples):  #@save\n    \"\"\"生成y=Xw+b+噪声\"\"\"\n    X = torch.normal(0, 1, (num_examples, len(w)))\n    y = torch.matmul(X, w) + b\n    y += torch.normal(0, 0.01, y.shape)\n    return X, y.reshape((-1, 1))\n\ntrue_w = torch.tensor([2, -3.4])\ntrue_b = torch.tensor(4.2)\n\nfeatures, labels = synthetic_data(true_w, true_b, 1000)\n\n\n\n调用torch.utils模块中的DataLoader方法读取数据集，它可以通过设定shuffle自动打乱顺序，同时返回批量为batch_size的可迭代对象。\n\ndef load_array(data_array, batch_size, is_train=True):\n    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n    dataset = data.TensorDataset(*data_array) # 传入的data_array是元组(X, y)，要先解包，它能将多个tensor对齐组成数据集。\n    return data.DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n\nbatch_size = 10\ndata_iter = load_array((features, labels), batch_size)\n\nnext(iter(data_iter))\n\n[tensor([[ 1.2391,  0.5222],\n         [ 0.6032, -0.7112],\n         [-0.3235, -0.7194],\n         [-0.8582,  0.7941],\n         [-0.3545,  0.7127],\n         [-0.1352, -2.4194],\n         [ 0.8341,  0.3817],\n         [-0.4976,  0.1100],\n         [ 0.1163, -1.0321],\n         [ 1.5996, -0.9930]]),\n tensor([[ 4.9098],\n         [ 7.8291],\n         [ 5.9892],\n         [-0.2285],\n         [ 1.0782],\n         [12.1557],\n         [ 4.5773],\n         [ 2.8291],\n         [ 7.9469],\n         [10.7816]])]\n\n\n\n\n\n首先定义一个模型变量net，它是一个Sequential类的实例。当给定输入数据时，它能将数据传入第一层，然后将第一次层的输出作为第二层的输入。。。\n\nfrom torch import nn\n\nnet = nn.Sequential(nn.Linear(2, 1)) # Linear方法接受两个参数，第一个参数指定输入特征的形状，第二个指定输出特征的形状。\n\n\n\n\n可以通过net[0]来选定第一个层，使用weight.data和bias.data访问参数，它会被初始化成随机值，可以使用normal_和fill_来重写参数。\n\nnet[0].weight.data.normal_(0, 0.01)\nnet[0].bias.data.fill_(0)\n\ntensor([0.])\n\n\n\n\n\npytorch中的均方损失是MSELoss类，它默认返回所有样本损失的平均值（由reduction指定mean、sum、none）。\n\nloss = nn.MSELoss()\n\n\n\n\n在pytorch的optim模块中有很多梯度下降算法的变种，这里使用SGD，它接收的params参数通过net.parameters()获得\n\ntrainer =torch.optim.SGD(net.parameters(), lr=0.03)\n\n\n\n对于每一个迭代周期里，通过迭代器不停的获取小批量的输入和相应的标签，对每一个小批量数据执行:\n\n通过调用net(X)生成预测并计算损失l（前向传播）。\n通过进行反向传播来计算梯度。\n通过调用优化器来更新模型参数。\n\n\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    for X, y in data_iter:\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        trainer.zero_grad() # 清空梯度\n        l.backward()\n        trainer.step() # 执行一步梯度下降，更新参数。\n    l = loss(net(features), labels) # 观察每一个迭代周期的训练效果\n    print(f'epoch {epoch+1}, loss {l:f}')\n\nw = net[0].weight.data\nprint('w的估计误差：', true_w - w.reshape(true_w.shape))\nb = net[0].bias.data\nprint('b的估计误差：', true_b - b)\n\nepoch 1, loss 0.000299\nepoch 2, loss 0.000102\nepoch 3, loss 0.000101\nw的估计误差： tensor([-8.7142e-04, -3.1710e-05])\nb的估计误差： tensor([0.0005])"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day3/index.html#softmax回归的简洁实现",
    "href": "posts/introduction-to-deep-learning-day3/index.html#softmax回归的简洁实现",
    "title": "深度学习入门(day3) - pytorch实现回归模型",
    "section": "",
    "text": "通过深度学习框架的高级API也能更方便地实现softmax回归模型。\n\nimport torch\nfrom torch import nn\nimport torchvision\nfrom torchvision import transforms\n\n\ndef get_dataloader_workers():\n    return 4\n\ndef load_data_fashion_mnist(batch_size, resize=None):\n    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n    trans = [transforms.ToTensor()]\n    if resize:\n        trans.insert(0, transforms.Resize(resize))\n    trans = transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=True, transform=trans, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(\n        root=\"./data\", train=False, transform=trans, download=True)\n    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n                            num_workers=get_dataloader_workers()),\n            data.DataLoader(mnist_test, batch_size, shuffle=False,\n                            num_workers=get_dataloader_workers()))\n\nbatch_size = 256\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size)\n\n\n\nsoftmax回归的输出层是一个全连接层。因此，只需在Sequential中添加一个带有10个输出的全连接层。\n\n# PyTorch不会隐式地调整输入的形状。因此，\n# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状\n# 通常第 0 维是批大小（batch_size），默认跳过，从第 1 维开始展平。\nnet = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights)\n\nSequential(\n  (0): Flatten(start_dim=1, end_dim=-1)\n  (1): Linear(in_features=784, out_features=10, bias=True)\n)\n\n\n\n\n\n对于之前的softmax函数 \\hat{y_i}=\\frac{e^{o_j}}{\\sum_{k}{e^{o_k}}}，如果 o_k 中的一些数值非常大，那么 e^{o_k} 可能会大过数据类型的上限，即上溢。可能会使分母变成inf，导致结果为0、inf或nan的 \\hat{y_j}。\n解决技巧是：先从所有 o_k 中减去 \\max{(o_k)}。\n\n\\begin{aligned}\n\\hat{y_i} &= \\frac{e^{o_j-\\max{(o_k)}}e^{\\max{(o_k)}}}{\\sum_{k}e^{o_k-\\max{(o_k)}}e^{\\max{(o_k)}}} \\\\\n&= \\frac{e^{o_j-\\max{(o_k)}}}{\\sum_{k}e^{o_k-\\max{(o_k)}}}\n\\end{aligned}\n\n经过以上步骤，可能有一些 o_j - \\max{(o_k)} 具有较大的负值。由于精度受限，e^{o_j - \\max{(o_k)}} 将会有接近零的值，即下溢。这些值可能会四舍五入为零，使 \\hat{y_i} 的值为 -\\infty。\n将softmax和交叉熵结合在一起，可以避免计算指数函数： \n\\begin{aligned}\n\\log{(\\hat{y_i})} &= \\log{\\left( \\frac{e^{o_j-\\max{(o_k)}}}{\\sum_{k}e^{o_k-\\max{(o_k)}}} \\right)} \\\\\n&= \\log{(e^{(o_j - \\max{(o_k)})})} - \\log{\\left( \\sum_{k}e^{o_k-\\max{(o_k)}} \\right)} \\\\\n&= o_j - \\max{(o_k)} - \\log{\\left( \\sum_{k}e^{o_k-\\max{(o_k)}} \\right)}\n\\end{aligned}\n\n最后在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数\n\n\"\"\"自动对模型的原始输出（logits，未归一化的分数）进行 LogSoftmax 处理，无需手动添加 Softmax 层。\"\"\"\nloss = nn.CrossEntropyLoss(reduction='none') \n\n\n\n\n使用学习率为0.1的小批量随机梯度下降作为优化算法。\n\ntrainer = torch.optim.SGD(net.parameters(), lr=0.1)\n\n\n\n\n先定义绘图工具类\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'  # 在 Jupyter 中设置矢量图显示\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\ndef accuracy(y_hat, y):\n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1: # y_hat 为矩阵\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y\n    return float(cmp.type(y.dtype).sum())\n\n\n# 先定义一个累加器工具\nclass Accumulator:\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n # 定义长度为n的列表\n    \n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)] # 列表与输入值累加\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx] # 重写[]方法\n\n\ndef evaluate_accuracy(net, data_iter):\n    \"\"\"计算在指定数据集上模型的精度\"\"\"\n    if isinstance(net, torch.nn.Module):\n        net.eval()  # 将模型设置为评估模式\n    metric = Accumulator(2)  # 正确预测数、预测总数\n    with torch.no_grad():\n        for X, y in data_iter:\n            metric.add(accuracy(net(X), y), y.numel())\n    return metric[0] / metric[1]\n\nclass Animator:\n    \"\"\"在动画中绘制数据\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        \n        # 创建图形和坐标轴\n        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes]  # 统一为列表格式方便后续操作\n            \n        # 坐标轴配置函数（原生替代d2l.set_axes）\n        def config_axes(ax):\n            ax.set_xlabel(xlabel)\n            ax.set_ylabel(ylabel)\n            if xlim: ax.set_xlim(xlim)\n            if ylim: ax.set_ylim(ylim)\n            ax.set_xscale(xscale)\n            ax.set_yscale(yscale)\n            if legend: ax.legend(legend)\n            ax.grid()\n            \n        self.config_axes = lambda: config_axes(self.axes[0])  # 绑定配置函数\n        \n        # 初始化数据容器\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        # 处理单值输入（兼容标量输入）\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        \n        # 处理x输入（兼容标量输入）\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n  # 所有y序列共享相同x值\n            \n        # 初始化数据存储列表\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n            \n        # 添加数据点\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        \n        # 清除当前图形并重绘\n        self.axes[0].cla()\n        for x_data, y_data, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x_data, y_data, fmt)\n            \n        # 配置坐标轴\n        self.config_axes()\n        \n        # 动态显示图形（需在Jupyter中运行）\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n再定义训练函数\n\ndef train_epoch(net, train_iter, loss, updater):  #@save\n    \"\"\"训练模型一个迭代周期\"\"\"\n    # 将模型设置为训练模式\n    if isinstance(net, torch.nn.Module):\n        net.train()\n    # 训练损失总和、训练准确度总和、样本数\n    metric = Accumulator(3)\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        if isinstance(updater, torch.optim.Optimizer):\n            # 使用PyTorch内置的优化器和损失函数\n            updater.zero_grad()\n            l.mean().backward()\n            updater.step()\n        else:\n            # 使用定制的优化器和损失函数\n            l.sum().backward()\n            updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练精度\n    return metric[0] / metric[2], metric[1] / metric[2]\n\ndef train(net, train_iter, test_iter, loss, num_epochs, updater):\n    \"\"\"训练模型\"\"\"\n    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n                        legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        train_metrics = train_epoch(net, train_iter, loss, updater)\n        test_acc = evaluate_accuracy(net, test_iter)\n        animator.add(epoch + 1, train_metrics + (test_acc,))\n    train_loss, train_acc = train_metrics\n    assert train_loss &lt; 0.5, train_loss\n    assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc\n    assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc\n\n开始训练\n\nnum_epoch = 10\ntrain(net, train_iter, test_iter, loss, num_epoch, trainer)"
  },
  {
    "objectID": "posts/introduction-to-deep-learning-day3/index.html#总结",
    "href": "posts/introduction-to-deep-learning-day3/index.html#总结",
    "title": "深度学习入门(day3) - pytorch实现回归模型",
    "section": "",
    "text": "使用深度学习框架的内置API，不仅可以简洁高效的训练模型，还额外采取预防措施避免数值异常。"
  }
]